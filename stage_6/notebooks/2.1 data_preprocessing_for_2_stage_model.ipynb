{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 5 (2.1 data_preprocessing_for_2_stage_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импортируем библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Optional\n",
    "\n",
    "import warnings\n",
    "\n",
    "# ----------------\n",
    "# Data processing\n",
    "# ----------------\n",
    "import dill\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импортируем пути"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/closed/\"\n",
    "models_path = \"../data/models/\"\n",
    "candidates_data_path = models_path + \"candidates_data/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfrorm ITEMS data for RANKER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного информации из df_items, а так же преобразуем данную таблицу (закодируем категориальные признаки)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>NAIVE QUERY PLAN</h4><p>run <b>LazyFrame.show_graph()</b> to see the optimized version</p><?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.1.2 (0)\n",
       " -->\n",
       "<!-- Title: polars_query Pages: 1 -->\n",
       "<svg width=\"286pt\" height=\"49pt\"\n",
       " viewBox=\"0.00 0.00 285.75 49.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 45)\">\n",
       "<title>polars_query</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-45 281.75,-45 281.75,4 -4,4\"/>\n",
       "<!-- p1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>p1</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"277.75,-41 0,-41 0,0 277.75,0 277.75,-41\"/>\n",
       "<text text-anchor=\"middle\" x=\"138.88\" y=\"-23.7\" font-family=\"Times,serif\" font-size=\"14.00\">Parquet SCAN [../data/closed/df_items.parquet]</text>\n",
       "<text text-anchor=\"middle\" x=\"138.88\" y=\"-7.2\" font-family=\"Times,serif\" font-size=\"14.00\">π */39;</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<LazyFrame at 0x7B3C35983450>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загружаем таблицу айтемов\n",
    "df_items = pl.scan_parquet(data_path + \"df_items.parquet\")\n",
    "df_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Schema([('item_id', Int64),\n",
       "        ('brand', String),\n",
       "        ('closure', String),\n",
       "        ('country', String),\n",
       "        ('cut', String),\n",
       "        ('height', String),\n",
       "        ('length', String),\n",
       "        ('material', String),\n",
       "        ('model', String),\n",
       "        ('neckline', String),\n",
       "        ('pattern', String),\n",
       "        ('pocket', String),\n",
       "        ('purpose', String),\n",
       "        ('sleeve', String),\n",
       "        ('color', Int64),\n",
       "        ('title_len', UInt32),\n",
       "        ('descr_len', UInt32),\n",
       "        ('title_word_len', UInt32),\n",
       "        ('descr_word_len', UInt32),\n",
       "        ('txt_emb_pca_0', Float64),\n",
       "        ('txt_emb_pca_1', Float64),\n",
       "        ('txt_emb_pca_2', Float64),\n",
       "        ('txt_emb_pca_3', Float64),\n",
       "        ('txt_emb_pca_4', Float64),\n",
       "        ('txt_emb_pca_5', Float64),\n",
       "        ('txt_emb_pca_6', Float64),\n",
       "        ('txt_emb_pca_7', Float64),\n",
       "        ('txt_emb_pca_8', Float64),\n",
       "        ('txt_emb_pca_9', Float64),\n",
       "        ('img_emb_pca_0', Float64),\n",
       "        ('img_emb_pca_1', Float64),\n",
       "        ('img_emb_pca_2', Float64),\n",
       "        ('img_emb_pca_3', Float64),\n",
       "        ('img_emb_pca_4', Float64),\n",
       "        ('img_emb_pca_5', Float64),\n",
       "        ('img_emb_pca_6', Float64),\n",
       "        ('img_emb_pca_7', Float64),\n",
       "        ('img_emb_pca_8', Float64),\n",
       "        ('img_emb_pca_9', Float64)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_items.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cat_features(df: pl.LazyFrame, cat_cols: List) -> Tuple[pl.DataFrame, OrdinalEncoder]:\n",
    "    \"\"\"\n",
    "    Function for enconding categorial features in table\n",
    "    and replacing values in original table\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit encoder\n",
    "    encoder = OrdinalEncoder(dtype=np.int64)\n",
    "    encoder.set_output(transform=\"polars\")\n",
    "    encoder.fit(df.select(cat_cols).collect())\n",
    "\n",
    "    # Modify DataFrame\n",
    "    return (\n",
    "        pl.concat(\n",
    "            [\n",
    "                df.drop(cat_cols).collect(),\n",
    "                encoder.transform(df.select(cat_cols).collect()),\n",
    "            ],\n",
    "            how=\"horizontal\",\n",
    "        ),\n",
    "        encoder,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_cat_cols = [\n",
    "    \"brand\",\n",
    "    \"color\",\n",
    "    \"closure\",\n",
    "    \"country\",\n",
    "    \"cut\",\n",
    "    \"height\",\n",
    "    \"length\",\n",
    "    \"material\",\n",
    "    \"model\",\n",
    "    \"neckline\",\n",
    "    \"pattern\",\n",
    "    \"pocket\",\n",
    "    \"purpose\",\n",
    "    \"sleeve\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_items_data(\n",
    "    items_path: str,\n",
    "    save_items_path: str,\n",
    "    save_enc_path: Optional[str] = None,\n",
    "):\n",
    "    \n",
    "    df_items = pl.scan_parquet(items_path)\n",
    "    df_items, items_can_enc = encode_cat_features(df_items, items_cat_cols)\n",
    "    df_items.write_parquet(save_items_path)\n",
    "\n",
    "    if save_enc_path:\n",
    "        with open(save_enc_path, \"wb\") as f:\n",
    "            dill.dump(items_can_enc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_items_data(\n",
    "    items_path=data_path + \"df_items.parquet\",\n",
    "    save_items_path=data_path + \"df_items_mod.parquet\",\n",
    "    save_enc_path=models_path + \"df_items_mod_encoder.dill\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Schema([('item_id', Int64),\n",
       "        ('title_len', UInt32),\n",
       "        ('descr_len', UInt32),\n",
       "        ('title_word_len', UInt32),\n",
       "        ('descr_word_len', UInt32),\n",
       "        ('txt_emb_pca_0', Float64),\n",
       "        ('txt_emb_pca_1', Float64),\n",
       "        ('txt_emb_pca_2', Float64),\n",
       "        ('txt_emb_pca_3', Float64),\n",
       "        ('txt_emb_pca_4', Float64),\n",
       "        ('txt_emb_pca_5', Float64),\n",
       "        ('txt_emb_pca_6', Float64),\n",
       "        ('txt_emb_pca_7', Float64),\n",
       "        ('txt_emb_pca_8', Float64),\n",
       "        ('txt_emb_pca_9', Float64),\n",
       "        ('img_emb_pca_0', Float64),\n",
       "        ('img_emb_pca_1', Float64),\n",
       "        ('img_emb_pca_2', Float64),\n",
       "        ('img_emb_pca_3', Float64),\n",
       "        ('img_emb_pca_4', Float64),\n",
       "        ('img_emb_pca_5', Float64),\n",
       "        ('img_emb_pca_6', Float64),\n",
       "        ('img_emb_pca_7', Float64),\n",
       "        ('img_emb_pca_8', Float64),\n",
       "        ('img_emb_pca_9', Float64),\n",
       "        ('brand', Int64),\n",
       "        ('color', Int64),\n",
       "        ('closure', Int64),\n",
       "        ('country', Int64),\n",
       "        ('cut', Int64),\n",
       "        ('height', Int64),\n",
       "        ('length', Int64),\n",
       "        ('material', Int64),\n",
       "        ('model', Int64),\n",
       "        ('neckline', Int64),\n",
       "        ('pattern', Int64),\n",
       "        ('pocket', Int64),\n",
       "        ('purpose', Int64),\n",
       "        ('sleeve', Int64)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.scan_parquet(data_path + \"df_items_mod.parquet\").schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_feature_engineering_with_interactions(\n",
    "    df: pl.LazyFrame | pl.DataFrame,\n",
    ") -> pl.LazyFrame | pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Function for calculating new features\n",
    "    \"\"\"\n",
    "    # переименуем для удобства\n",
    "    df = df.rename(\n",
    "        {\n",
    "            \"u_total_inter\": \"user_hist\",\n",
    "        }\n",
    "    )\n",
    "    # Получаем популярность контента\n",
    "    df = df.with_columns(pl.col(\"user_id\").count().over(\"item_id\").alias(\"item_pop\"))\n",
    "\n",
    "    # Получаем среднюю популярность контента, просматриваемого этим юзером\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"item_pop\").mean().over(\"user_id\").alias(\"user_avg_pop\")\n",
    "    )\n",
    "\n",
    "    # Получаем среднюю длину истории пользователя, которые смотрит этот контент\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"user_hist\").mean().over(\"item_id\").alias(\"item_avg_hist\")\n",
    "    )\n",
    "\n",
    "    # Получаем популярность последнего просмотренного контента\n",
    "    df = df.sort(\n",
    "        by=[\"user_id\", \"dt\"],\n",
    "        descending=[False, True],\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"item_pop\").first().over(\"user_id\").alias(\"user_last_pop\")\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tables_with_users_and_items_features(\n",
    "    interactions_path: str,\n",
    "    users_path: Optional[str] = None,\n",
    "    items_path: Optional[str] = None,\n",
    "    save_users_path: Optional[str] = None,\n",
    "    save_items_path: Optional[str] = None,\n",
    "):\n",
    "    interactions = pl.scan_parquet(interactions_path)\n",
    "\n",
    "    if users_path:\n",
    "        df_users = pl.scan_parquet(users_path)\n",
    "    elif save_users_path:\n",
    "        df_users = interactions.select(\"user_id\").unique()\n",
    "    else:\n",
    "        raise \"users_path or save_users_path should be passed to funciton\"\n",
    "\n",
    "    if items_path:\n",
    "        df_items = pl.scan_parquet(items_path)\n",
    "    elif save_items_path:\n",
    "        df_items = interactions.select(\"item_id\").unique()\n",
    "    else:\n",
    "        raise \"users_path or save_users_path should be passed to funciton\"\n",
    "\n",
    "    # Добавляем новые фичи в соответствующие таблицы\n",
    "    df_items.join(\n",
    "        other=func_feature_engineering_with_interactions(interactions)\n",
    "        .select([\"item_id\", \"item_pop\", \"item_avg_hist\"])\n",
    "        .unique(),\n",
    "        how=\"left\",\n",
    "        on=\"item_id\",\n",
    "    ).fill_null(0).collect().write_parquet(save_items_path)\n",
    "\n",
    "    # Создаем таблицу с фитчами пользователей\n",
    "    df_users.join(\n",
    "        other=func_feature_engineering_with_interactions(interactions)\n",
    "        .select([\"user_id\", \"user_hist\", \"user_avg_pop\", \"user_last_pop\"])\n",
    "        .unique(),\n",
    "        how=\"left\",\n",
    "        on=\"user_id\",\n",
    "    ).fill_null(0).collect().write_parquet(save_users_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tables_with_users_and_items_features(\n",
    "    interactions_path=data_path + \"base_models_data.parquet\",\n",
    "    users_path=None,\n",
    "    items_path=data_path + \"df_items_mod.parquet\",\n",
    "    save_users_path=data_path + \"df_users.parquet\",\n",
    "    save_items_path=data_path + \"df_items_mod.parquet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker_data = pl.scan_parquet(data_path + \"ranker_data.parquet\")\n",
    "candidates_full = pl.scan_parquet(candidates_data_path + \"candidates_full.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Schema([('user_id', Int64),\n",
       "         ('item_id', Int64),\n",
       "         ('dt', Datetime(time_unit='ns', time_zone=None)),\n",
       "         ('ui_inter', UInt32),\n",
       "         ('u_total_inter', UInt32),\n",
       "         ('weight', Float64),\n",
       "         ('ui_entry', Int64),\n",
       "         ('cum_weight', Float64)]),\n",
       " Schema([('user_id', Int64),\n",
       "         ('item_id', Int64),\n",
       "         ('cos_score', Float32),\n",
       "         ('cos_rank', Int64),\n",
       "         ('bm25_score', Float32),\n",
       "         ('bm25_rank', Int64),\n",
       "         ('tfidf_score', Float32),\n",
       "         ('tfidf_rank', Int64),\n",
       "         ('lfm_score', Float32),\n",
       "         ('lfm_rank', Int64)]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker_data.schema, candidates_full.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пользователи, которым надо выдавать пресказания для обучения ранкера,\n",
    "# т.е. присутствуют и в base_models_data и в ranker_data (base to ranker users)\n",
    "with open(data_path + \"b2r_users.dill\", \"rb\") as f:\n",
    "    b2r_users = dill.load(f)\n",
    "\n",
    "\n",
    "# Пользователи из test_df, которым будут выданы\n",
    "# таргетирвонные рекомондации\n",
    "with open(data_path + \"bNr2t_users.dill\", \"rb\") as f:\n",
    "    bNr2t_users = dill.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_list = [\"cos\", \"bm25\", \"tfidf\", \"lfm\"]\n",
    "default_values_candidates = {}\n",
    "for cand in candidates_list:\n",
    "    default_values_candidates[f\"{cand}_score\"] = (\n",
    "        candidates_full.select(f\"{cand}_score\").min().collect().item()\n",
    "    )\n",
    "    default_values_candidates[f\"{cand}_rank\"] = (\n",
    "        candidates_full.select(f\"{cand}_rank\").max().collect().item()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cos_score': -0.009999113157391548,\n",
       " 'cos_rank': 15,\n",
       " 'bm25_score': -0.009771198034286499,\n",
       " 'bm25_rank': 15,\n",
       " 'tfidf_score': -0.009981262497603893,\n",
       " 'tfidf_rank': 15,\n",
       " 'lfm_score': -5.932222843170166,\n",
       " 'lfm_rank': 15}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_values_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель второго уровня (ранкер)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranker Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оставим только необходимые параметры из таблицы\n",
    "\n",
    "# Ранкер будем обучать на пользователях у кого длинная история взаимодействий\n",
    "ranker_data = ranker_data.filter(pl.col(\"u_total_inter\") > 75).select(\n",
    "    [\n",
    "        \"user_id\",\n",
    "        \"item_id\",\n",
    "        # Так как бьем данные для tain val не по времени,\n",
    "        # колонка \"dt\" не нужна\n",
    "        # --------------------------\n",
    "        # Потом будем использовать для ранкера чтобы задать таргет\n",
    "        # (количество взаимодействий с предметом)\n",
    "        \"ui_inter\",\n",
    "        # --------------------------\n",
    "        # Веса\n",
    "        # \"weight\",\n",
    "        # \"cum_weight\",\n",
    "        # \"rel_weight\"\n",
    "        # Убираем, т.к. они были получены из схожих соображений\n",
    "        # и зависят от +- одинаковых фитчей\n",
    "        # А на \"rel_weight\" обучалась модель первого уровня\n",
    "        # так что далее он не нужен\n",
    "        # --------------------------\n",
    "        # Остальные колонки не нужны\n",
    "        # Так как они были использованы для вывода весовых колонок,\n",
    "        # либо присутствуют в фитчах пользователя или айтема\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>NAIVE QUERY PLAN</h4><p>run <b>LazyFrame.show_graph()</b> to see the optimized version</p><?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.1.2 (0)\n",
       " -->\n",
       "<!-- Title: polars_query Pages: 1 -->\n",
       "<svg width=\"302pt\" height=\"193pt\"\n",
       " viewBox=\"0.00 0.00 301.50 193.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 189)\">\n",
       "<title>polars_query</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-189 297.5,-189 297.5,4 -4,4\"/>\n",
       "<!-- p1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>p1</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"173.75,-185 119.75,-185 119.75,-149 173.75,-149 173.75,-185\"/>\n",
       "<text text-anchor=\"middle\" x=\"146.75\" y=\"-161.95\" font-family=\"Times,serif\" font-size=\"14.00\">π 3/3</text>\n",
       "</g>\n",
       "<!-- p2 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>p2</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"271.75,-113 21.75,-113 21.75,-77 271.75,-77 271.75,-113\"/>\n",
       "<text text-anchor=\"middle\" x=\"146.75\" y=\"-89.95\" font-family=\"Times,serif\" font-size=\"14.00\">FILTER BY [(col(&quot;u_total_inter&quot;)) &gt; (75)]</text>\n",
       "</g>\n",
       "<!-- p1&#45;&#45;p2 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>p1&#45;&#45;p2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M146.75,-148.7C146.75,-137.85 146.75,-123.92 146.75,-113.1\"/>\n",
       "</g>\n",
       "<!-- p3 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>p3</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"293.5,-41 0,-41 0,0 293.5,0 293.5,-41\"/>\n",
       "<text text-anchor=\"middle\" x=\"146.75\" y=\"-23.7\" font-family=\"Times,serif\" font-size=\"14.00\">Parquet SCAN [../data/closed/ranker_data.parquet]</text>\n",
       "<text text-anchor=\"middle\" x=\"146.75\" y=\"-7.2\" font-family=\"Times,serif\" font-size=\"14.00\">π */8;</text>\n",
       "</g>\n",
       "<!-- p2&#45;&#45;p3 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>p2&#45;&#45;p3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M146.75,-76.82C146.75,-66.18 146.75,-52.49 146.75,-41.38\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<LazyFrame at 0x71264A332CD0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train \\ Val \\ Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь ranker_data разбиваем по юзерам\n",
    "# на train и val для обучения и валидации ранкера\n",
    "train_size = 0.8\n",
    "val_size = 0.2\n",
    "\n",
    "\n",
    "ranker_train_users, ranker_val_users = train_test_split(\n",
    "    ranker_data.select(\"user_id\")\n",
    "    .filter(pl.col(\"user_id\").is_in(b2r_users))\n",
    "    .collect()\n",
    "    .to_numpy()\n",
    "    .flatten(),\n",
    "    random_state=RANDOM_STATE,\n",
    "    test_size=val_size,\n",
    ")\n",
    "\n",
    "# test-выборка у нас уже имеется\n",
    "# выборка пользователей присутствующих в base & ranker & test\n",
    "# на них и будем проводить первичный тест системы\n",
    "ranker_test_users = bNr2t_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оставляем среди users только тех, для кого есть\n",
    "# и рекомендации и таргеты\n",
    "def users_filter(\n",
    "    user_list: np.ndarray,\n",
    "    candidates_df: pl.LazyFrame,\n",
    "    df: pl.LazyFrame,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters user interaction data and candidate recommendations,\n",
    "    ensuring each user has both interactions and recommendations.\n",
    "\n",
    "    Args:\n",
    "        user_list (np.ndarray): User IDs to include.\n",
    "        candidates_df (pl.LazyFrame): Candidate item recommendations\n",
    "            with ranks ('cos_rank', 'bm25_rank', 'lfm_rank', 'tfidf_rank').\n",
    "        df (pl.LazyFrame): User-item interactions ('user_id', 'item_id', 'dt',\n",
    "            and potentially other weight-based columns).\n",
    "\n",
    "    Returns:\n",
    "        pl.LazyFrame: Filtered and merged DataFrame with user interactions\n",
    "            and candidate items sorted and with missing values filled.\n",
    "            It also filters down to items with at least one rank < 15\n",
    "    \"\"\"\n",
    "    # For fillna\n",
    "    default_values = {\n",
    "        \"ui_inter\": 0,\n",
    "        **default_values_candidates,\n",
    "    }\n",
    "\n",
    "    # Get valid interactions\n",
    "    df = df.filter(pl.col(\"user_id\").is_in(user_list))\n",
    "    candidates_df = candidates_df.filter(pl.col(\"user_id\").is_in(user_list))\n",
    "\n",
    "    # join interaction на наших кандидатов для users из train, val, test\n",
    "    df = (\n",
    "        df.join(\n",
    "            other=candidates_df,\n",
    "            how=\"outer\",\n",
    "            on=[\"user_id\", \"item_id\"],\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col(\"user_id\").fill_null(pl.col(\"user_id_right\")),\n",
    "            pl.col(\"item_id\").fill_null(pl.col(\"item_id_right\")),\n",
    "        )\n",
    "        .drop([\"user_id_right\", \"item_id_right\"])\n",
    "    )\n",
    "    df = df.collect().with_columns(\n",
    "        (\n",
    "            pl.col(col_name).fill_null(default_values[col_name])\n",
    "            for col_name in default_values.keys()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Сортируем по user_id\n",
    "    df = df.sort(by=[\"user_id\", \"item_id\"])\n",
    "    df = df.filter(\n",
    "        (pl.col(\"cos_rank\") < 15)\n",
    "        | (pl.col(\"bm25_rank\") < 15)\n",
    "        | (pl.col(\"lfm_rank\") < 15)\n",
    "        | (pl.col(\"tfidf_rank\") < 15)\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_filter(ranker_train_users, candidates_full, ranker_data).write_parquet(\n",
    "    data_path + \"ranker_train.parquet\"\n",
    ")\n",
    "users_filter(ranker_val_users, candidates_full, ranker_data).write_parquet(\n",
    "    data_path + \"ranker_val.parquet\"\n",
    ")\n",
    "users_filter(ranker_test_users, candidates_full, ranker_data).write_parquet(\n",
    "    data_path + \"ranker_test.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Schema([('user_id', Int64),\n",
       "        ('item_id', Int64),\n",
       "        ('ui_inter', UInt32),\n",
       "        ('cos_score', Float32),\n",
       "        ('cos_rank', Int64),\n",
       "        ('bm25_score', Float32),\n",
       "        ('bm25_rank', Int64),\n",
       "        ('tfidf_score', Float32),\n",
       "        ('tfidf_rank', Int64),\n",
       "        ('lfm_score', Float32),\n",
       "        ('lfm_rank', Int64)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.scan_parquet(data_path + \"ranker_train.parquet\").schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Добавим фитчи предметов и пользователей "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пользователей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем таблицу фитчей пользователей\n",
    "df_users = pl.scan_parquet(data_path + \"df_users.parquet\")\n",
    "\n",
    "\n",
    "# Для новых фичей юзеров\n",
    "default_values_users = {\n",
    "    \"user_hist\": 0,\n",
    "    \"user_avg_pop\": df_users.select(\"user_avg_pop\").median().collect().item(),\n",
    "    \"user_last_pop\": df_users.select(\"user_last_pop\").median().collect().item(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавляем фичи\n",
    "def add_users_features(\n",
    "    df: pl.LazyFrame,\n",
    "    users: pl.LazyFrame,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Merges user and item features into a DataFrame, handling missing values.\n",
    "\n",
    "    Args:\n",
    "        df (pl.LazyFrame): Interaction DataFrame ('user_id', 'item_id').\n",
    "        users (pl.LazyFrame): User features DataFrame ('user_id').\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame with merged user and item features,\n",
    "            and missing values filled.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.join(\n",
    "        other=users.filter(\n",
    "            pl.col(\"user_id\").is_in(df.select(\"user_id\").unique().collect())\n",
    "        ),\n",
    "        how=\"left\",\n",
    "        on=[\"user_id\"],\n",
    "    )\n",
    "\n",
    "    # При джойне могут получиться строки\n",
    "    # с несуществующими айтемами или юзерами.\n",
    "    # Заполняем пропуски\n",
    "    return df.collect().with_columns(\n",
    "        (\n",
    "            pl.col(col_name).fill_null(default_values_users[col_name])\n",
    "            for col_name in default_values_users.keys()\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_users_features(\n",
    "    pl.scan_parquet(data_path + \"ranker_train.parquet\"), df_users\n",
    ").write_parquet(data_path + \"ranker_train.parquet\")\n",
    "\n",
    "add_users_features(\n",
    "    pl.scan_parquet(data_path + \"ranker_val.parquet\"), df_users\n",
    ").write_parquet(data_path + \"ranker_val.parquet\")\n",
    "\n",
    "add_users_features(\n",
    "    pl.scan_parquet(data_path + \"ranker_test.parquet\"), df_users\n",
    ").write_parquet(data_path + \"ranker_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Schema([('user_id', Int64),\n",
       "        ('item_id', Int64),\n",
       "        ('ui_inter', UInt32),\n",
       "        ('cos_score', Float32),\n",
       "        ('cos_rank', Int64),\n",
       "        ('bm25_score', Float32),\n",
       "        ('bm25_rank', Int64),\n",
       "        ('tfidf_score', Float32),\n",
       "        ('tfidf_rank', Int64),\n",
       "        ('lfm_score', Float32),\n",
       "        ('lfm_rank', Int64),\n",
       "        ('user_hist', UInt32),\n",
       "        ('user_avg_pop', Float64),\n",
       "        ('user_last_pop', Float64)])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.scan_parquet(data_path + \"ranker_train.parquet\").schema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предметов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем таблицу айтемов\n",
    "df_items = pl.scan_parquet(data_path + \"df_items_mod.parquet\")\n",
    "\n",
    "# Для новых фичей айтемов\n",
    "default_values_items = {\n",
    "    \"item_pop\": df_items.select(\"item_pop\").median().collect().item(),\n",
    "    \"item_avg_hist\": df_items.select(\"item_avg_hist\").median().collect().item(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавляем фичи\n",
    "def add_items_features(\n",
    "    df: pl.LazyFrame,\n",
    "    items: pl.LazyFrame,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Merges user and item features into a DataFrame, handling missing values.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Interaction DataFrame ('user_id', 'item_id').\n",
    "        items (pd.DataFrame): Item features DataFrame ('item_id').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with merged user and item features,\n",
    "            and missing values filled.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.join(\n",
    "        other=items.filter(\n",
    "            pl.col(\"item_id\").is_in(df.select(\"item_id\").unique().collect())\n",
    "        ),\n",
    "        how=\"left\",\n",
    "        on=[\"item_id\"],\n",
    "    )\n",
    "\n",
    "    # При джойне могут получиться строки\n",
    "    # с несуществующими айтемами или юзерами.\n",
    "    # Заполняем пропуски\n",
    "    return df.collect().with_columns(\n",
    "        (\n",
    "            pl.col(col_name).fill_null(default_values_items[col_name])\n",
    "            for col_name in default_values_items.keys()\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_items_features(\n",
    "    pl.scan_parquet(data_path + \"ranker_train.parquet\"), df_items\n",
    ").write_parquet(data_path + \"ranker_train.parquet\")\n",
    "\n",
    "add_items_features(\n",
    "    pl.scan_parquet(data_path + \"ranker_val.parquet\"), df_items\n",
    ").write_parquet(data_path + \"ranker_val.parquet\")\n",
    "\n",
    "add_items_features(\n",
    "    pl.scan_parquet(data_path + \"ranker_test.parquet\"), df_items\n",
    ").write_parquet(data_path + \"ranker_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Добавим таргет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target(df: pl.LazyFrame) -> pl.DataFrame:\n",
    "\n",
    "    return df.with_columns(\n",
    "        pl.when(pl.col(\"ui_inter\") > 6)\n",
    "        .then(10)\n",
    "        .when(pl.col(\"ui_inter\") > 4)\n",
    "        .then(8)\n",
    "        .when(pl.col(\"ui_inter\") > 2)\n",
    "        .then(4)\n",
    "        .when(pl.col(\"ui_inter\") > 1)\n",
    "        .then(2)\n",
    "        .otherwise(1)\n",
    "        .alias(\"target\")\n",
    "    ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_target(pl.scan_parquet(data_path + \"ranker_train.parquet\")).write_parquet(\n",
    "    data_path + \"ranker_train.parquet\"\n",
    ")\n",
    "add_target(pl.scan_parquet(data_path + \"ranker_val.parquet\")).write_parquet(\n",
    "    data_path + \"ranker_val.parquet\"\n",
    ")\n",
    "add_target(pl.scan_parquet(data_path + \"ranker_test.parquet\")).write_parquet(\n",
    "    data_path + \"ranker_test.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|Stage|Description|Time (s)|\n",
    "|---|---|---|\n",
    "|Item Data Encoding|Encoding categorical features in the items data|4|\n",
    "|Feature Engineering|Creating new features|17.4|\n",
    "|Train/Validation Split|Splitting the data into training and validation sets|1|\n",
    "|Users Filter (Merge Interactions)|Applying a filter on users (merging with candidates table)|13.6|\n",
    "|Merging with Users’ Features|Merging with a table containing user features|2.8|\n",
    "|Merging with Items’ Features|Merging with a table containing item features|17.9|\n",
    "|Adding Target Variable|Adding the target variable (likely a dependent variable)|17.2|\n",
    "|**Total Preprocessing Time**|**Total time for all preprocessing steps**|**74.0**|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
