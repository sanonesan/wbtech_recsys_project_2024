{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WB RecSys Project\n",
    "\n",
    "# Общее описание проекта\n",
    "\n",
    "Необходимо на основании взаимодействий пользователей с товарами предсказать следующие взаимодействия пользователей с товарами.\n",
    "\n",
    "# Stage 3\n",
    "\n",
    "- Сформировать обучающую выборку\n",
    "- Спроектировать схему валидации с учетом специфики задачи\n",
    "- Обосновать выбор способа валидации\n",
    "\n",
    "\n",
    "# Preprocessing images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорт библиотек"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import dill\n",
    "import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Путь до данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../data/closed/\"\n",
    "data_load_path = \"../../data/load/\"\n",
    "img_data_path = data_load_path + \"images/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтение данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим для каких айтемов отсутствуют картинки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorded list with existing items\n",
    "item_id_list = (\n",
    "    pl.scan_parquet(data_path + \"df_items.parquet\")\n",
    "    .select(\"item_id\")\n",
    "    .sort(by=\"item_id\")\n",
    "    .collect()\n",
    "    .to_numpy()\n",
    "    .flatten()\n",
    "    .tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_without_img = []\n",
    "\n",
    "for i in item_id_list:\n",
    "    if not (os.path.exists(img_data_path + f\"{i}.jpg\")):\n",
    "        items_without_img.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(items_without_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всего таких айтемов 269"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(img_data_path, f\"{1}.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP: Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получим отсортированный список лейблов изображений,\n",
    "# для которых присутствуют картиики\n",
    "labels = sorted(list(set(item_id_list) - set(items_without_img)))\n",
    "\n",
    "images_paths = [\n",
    "    img_data_path + f\"{idx}.jpg\" for idx in labels\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Размер изображения, к которому будем приводить\n",
    "IMG_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зададим свой датасет\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_paths,\n",
    "        labels,\n",
    "        transform=None,\n",
    "        image_size: int= 128,\n",
    "    ):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.file_paths[index]).resize(\n",
    "            size=(self.image_size, self.image_size),\n",
    "        )\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем датасет\n",
    "imgs_dataset = CustomImageDataset(\n",
    "    images_paths, #[:128  * 20],\n",
    "    labels=labels, #[:128  * 20],\n",
    "    image_size=IMG_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image embeddings\n",
    "\n",
    "Extract image embeddings on both subsets using pretrained CLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model\n",
    "model = CLIPModel.from_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\",\n",
    ").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\",\n",
    ")\n",
    "\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgs_collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    images = [\n",
    "        processor(\n",
    "            images=image,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"pixel_values\"]\n",
    "        for image in images\n",
    "    ]\n",
    "    images = torch.cat(images, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "# Get dataloader\n",
    "imgs_loader = torch.utils.data.DataLoader(\n",
    "    imgs_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=imgs_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(\n",
    "    model, loader, device=device\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Отключаем градиенты для ускорения\n",
    "        for images, labels in tqdm.tqdm(iter(loader)):\n",
    "            # Перемещаем данные на указанное устройство\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Извлекаем эмбеддинги из модели\n",
    "            embeddings = model.get_image_features(images)\n",
    "\n",
    "            # Сохраняем результаты\n",
    "            all_embeddings.append(embeddings)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    # Объединяем все батчи в один тензор\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0).to(\"cpu\")\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    return all_embeddings, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embeds, img_labels = extract_embeddings(model, imgs_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_labels, img_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embeds, img_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраним в бинарник эмбединги катинок\n",
    "with open(data_path + \"img_embeds.dill\", \"wb\") as f:\n",
    "    dill.dump(img_embeds, f)\n",
    "\n",
    "# Сохраним в бинарник лейблы картинок\n",
    "with open(data_path + \"img_labels.dill\", \"wb\") as f:\n",
    "    dill.dump(img_labels, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Снизим размерность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим данные\n",
    "with open(data_path + \"img_embeds.dill\", \"rb\") as f:\n",
    "    img_embeds = dill.load(f)\n",
    "\n",
    "# Загрузим данные\n",
    "with open(data_path + \"img_labels.dill\", \"rb\") as f:\n",
    "    img_labels = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_to_keep = 10\n",
    "\n",
    "pca_lowrank = PCA(n_components=components_to_keep)\n",
    "all_embeddings = pca_lowrank.fit_transform(img_embeds)\n",
    "all_embeddings.shape\n",
    "img_embeds_df = pl.concat(\n",
    "    [\n",
    "        pl.DataFrame(\n",
    "            data=img_labels.numpy(),\n",
    "            schema=[\"item_id\"],\n",
    "        ),\n",
    "        pl.DataFrame(\n",
    "            data=all_embeddings,\n",
    "            schema=[f\"img_emb_pca_{i}\" for i in range(components_to_keep)],\n",
    "        ),\n",
    "    ],\n",
    "    how=\"horizontal\",\n",
    ")\n",
    "\n",
    "img_embeds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сольем данные в одну таблицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.scan_parquet(data_path + \"df_items.parquet\")\n",
    "    .collect()\n",
    "    .join(other=img_embeds_df, on=\"item_id\", how=\"left\")\n",
    "    # Заполняем пропуски товаров, для которых нет изображений\n",
    "    .fill_nan(0)\n",
    "    .fill_null(0)\n",
    ").write_parquet(data_path + \"df_items.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.scan_parquet(data_path + \"df_items.parquet\").schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
