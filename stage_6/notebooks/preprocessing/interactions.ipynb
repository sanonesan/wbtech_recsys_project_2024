{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WB RecSys Project\n",
    "\n",
    "# Общее описание проекта\n",
    "\n",
    "Необходимо на основании взаимодействий пользователей с товарами предсказать следующие взаимодействия пользователей с товарами.\n",
    "\n",
    "# Stage 3\n",
    "\n",
    "- Сформировать обучающую выборку\n",
    "- Спроектировать схему валидации с учетом специфики задачи\n",
    "- Обосновать выбор способа валидации\n",
    "\n",
    "\n",
    "# Preprocessing train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import dill\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Путь до данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../data/closed/\"\n",
    "data_load_path = \"../../data/load/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтение "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.scan_parquet(data_load_path + \"train_data_10_10_24_10_11_24_final.parquet\").schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = (\n",
    "    pl.scan_parquet(data_load_path + \"train_data_10_10_24_10_11_24_final.parquet\")\n",
    "    # Отбираем необходимые колонки\n",
    "    .select([\"wbuser_id\", \"nm_id\", \"dt\"])\n",
    "    # Отсортируем по дате\n",
    "    .sort(by=\"dt\")\n",
    "    # Для удобства переименуем колонки\n",
    "    .rename(\n",
    "        {\n",
    "            \"wbuser_id\": \"user_id\",\n",
    "            \"nm_id\": \"item_id\",\n",
    "        }\n",
    "    )\n",
    "    # Выполняем\n",
    "    .collect()\n",
    ")\n",
    "interactions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = interactions_df[\"dt\"].min()\n",
    "max_date = interactions_df[\"dt\"].max()\n",
    "\n",
    "print(f'min_date = {min_date.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "print(f'max_date = {max_date.strftime(\"%Y-%m-%d %H:%M:%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Посмотрим распределение покупок по часам для каждого из дней\n",
    "\n",
    "# min_date = interactions_df[\"dt\"].min()\n",
    "# max_date = interactions_df[\"dt\"].max()\n",
    "\n",
    "# n_days = (max_date - min_date).days + 1\n",
    "\n",
    "# fig, ax = plt.subplots(1, n_days, figsize=(18, 6), sharey=True)\n",
    "\n",
    "# for i_day in range(n_days):\n",
    "\n",
    "#     cur_day = min_date + timedelta(days=i_day)\n",
    "#     next_day = cur_day + timedelta(days=1)\n",
    "\n",
    "#     cur_day = datetime(year=cur_day.year, month=cur_day.month, day=cur_day.day)\n",
    "#     next_day = datetime(year=next_day.year, month=next_day.month, day=next_day.day)\n",
    "\n",
    "#     days = (\n",
    "#         interactions_df[\"dt\"]\n",
    "#         .filter(interactions_df[\"dt\"].is_between(cur_day, next_day))\n",
    "#         .value_counts()\n",
    "#         .group_by_dynamic(\"dt\", every=\"1h\")\n",
    "#         .agg(pl.col(\"count\").sum())\n",
    "#         .sort(by=\"dt\")\n",
    "#     )\n",
    "\n",
    "#     sns.barplot(\n",
    "#         x=days[\"dt\"].dt.hour(),\n",
    "#         y=days[\"count\"],\n",
    "#         ax=ax[i_day],\n",
    "#     )\n",
    "\n",
    "#     ax[i_day].set_title(f\"Распределение заказов {cur_day.strftime('%Y-%m-%d')}\")\n",
    "#     ax[i_day].set_xlabel(\"Время, часы\")\n",
    "#     ax[i_day].set_ylabel(\"Число заказов\")\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Разделим данные на train \\ test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Конечная дата\n",
    "max_date = interactions_df[\"dt\"].max()\n",
    "\n",
    "# Дата начала данных для теста\n",
    "train_test_sep = max_date - timedelta(hours=8)\n",
    "\n",
    "# Данные для теста\n",
    "test_df = interactions_df.filter(interactions_df[\"dt\"] >= train_test_sep)\n",
    "\n",
    "# Данные для обучения моделей первого \n",
    "# и второго уровня (разделение будет потом)\n",
    "train_df = interactions_df.filter(interactions_df[\"dt\"] < train_test_sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним в parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.write_parquet(data_path + \"train_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Соберем test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем следующие просмотренные товары в списки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Таблица взаимодействий уже была заранее отсортирована по дате,\n",
    "# так что порядок взаимодействий по дате сохранится \n",
    "test_df = test_df.group_by(\"user_id\").agg(pl.col(\"item_id\")).sort(by=\"user_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь следующий вопрос: сколько товаров рекомендовать? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Информации о длинах интеракций пользователей в test_df\n",
    "test_df[\"item_id\"].map_elements(len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составим отранжированный список товаров \n",
    "\n",
    "> по сути необходимо только для замера метрик, а для продакшена необходимы будут только айдишники пользователей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_item_id_test_df(x: pl.List):\n",
    "    \"\"\"Formats item IDs based on frequency within a user's viewing history using Polars.\"\"\"\n",
    "    if len(x) > 1:\n",
    "        return x.value_counts().sort(\"count\", descending=True)[\"\"]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.with_columns(pl.col(\"item_id\").map_elements(format_item_id_test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним в parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.write_parquet(data_path + \"test_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Модифицируем только таблицу train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_weights = (\n",
    "    pl.scan_parquet(data_path + \"train_df.parquet\").group_by([\"user_id\", \"item_id\"])\n",
    "    # Посчитаем количество взаимодействий пользователя\n",
    "    # с каждым конкретным товаром\n",
    "    .agg(pl.col(\"item_id\").count().alias(\"ui_inter\"))\n",
    ")\n",
    "\n",
    "train_df_weights = train_df_weights.join(\n",
    "    train_df_weights.select([\"user_id\", \"ui_inter\"])\n",
    "    .group_by(\"user_id\")\n",
    "    .agg(pl.col(\"ui_inter\").sum().alias(\"u_total_inter\")),\n",
    "    on=\"user_id\",\n",
    "    how=\"left\",\n",
    ").with_columns((pl.col(\"ui_inter\") / pl.col(\"u_total_inter\")).alias(\"weight\"))\n",
    "\n",
    "train_df_weights.collect().write_parquet(data_path + \"train_df_weights.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем количество взаимодействий с определенным товаром (`item_count`)\n",
    "на его основе расчитаем рейтинг товара (`item_rating`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Все взаимодействия с каждым товаром\n",
    "item_rating_df = (\n",
    "    pl.scan_parquet(data_path + \"train_df_weights.parquet\")\n",
    "    .select([\"item_id\", \"ui_inter\"])\n",
    "    .group_by(\"item_id\")\n",
    "    .agg(pl.col(\"ui_inter\").sum().alias(\"item_count\"))\n",
    "    .collect()\n",
    ")\n",
    "# Общий вес\\рейтинг товара по всем пользователям\n",
    "item_rating_df = item_rating_df.with_columns(\n",
    "    (pl.col(\"item_count\") / item_rating_df.shape[0]).alias(\"item_rating\")\n",
    ").sort(by=\"item_rating\", descending=True)\n",
    "\n",
    "item_rating_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним таблицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_rating_df.write_parquet(data_path + \"item_rating_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Мерджим таблицу с весами к основной\n",
    "\n",
    "Теперь будем присоединять веса к общей таблице (где существуют данные о дате взаимодействия)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pl.scan_parquet(data_path + \"train_df.parquet\")\n",
    "train_df_weights = pl.scan_parquet(data_path + \"train_df_weights.parquet\")\n",
    "\n",
    "# Иерджим и сохраняем в parquet\n",
    "train_df.join(\n",
    "    train_df_weights,\n",
    "    on=[\"user_id\", \"item_id\"],\n",
    "    how=\"left\",\n",
    ").collect().write_parquet(data_path + \"train_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Добавляем новые фитчи к train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.scan_parquet(data_path + \"train_df.parquet\")\n",
    "    # Подсчет порядковых номеров взаимодействия пользователя с \n",
    "    # каждым конкретынм товаром\n",
    "    #\n",
    "    # Т.е. если взаимодействия с товарами идут в следующем порядке\n",
    "    # [1, 2, 1, 1, 2, 4]\n",
    "    #\n",
    "    # то результат будет следующим:\n",
    "    #\n",
    "    # cumcount([1, 2, 1, 1, 2, 4]) + 1 = [1, 1, 2, 3, 2, 1]\n",
    "    #\n",
    "    # Можно расчитать быстрее, используя оконную функцию аналогично предыдущему запросу\n",
    "    # Но у меня не умещается такой вариант в оперативной памяти\n",
    "    .with_columns(\n",
    "        (pl.int_range(1, pl.len() + 1)).over([\"user_id\", \"item_id\"]).alias(\"ui_entry\")\n",
    "    )\n",
    "    # кумулятивный вес товара на момент просмотра\n",
    "    .with_columns(\n",
    "        (pl.col(\"ui_entry\") / pl.col(\"ui_inter\") * pl.col(\"weight\")).alias(\"cum_weight\")\n",
    "    )\n",
    "    # сортировать не обязательно, но хорошо будет\n",
    "    # если отсортировать по времени, т.к. в дальнейшем \n",
    "    # записи будут делиться по времени и сортировка ускорит процесс:\n",
    "    # predict блок в в процессоре не будет \"спотыкаться\"\n",
    "    .sort(by=[\"dt\"])\n",
    "    .collect()\n",
    "# сразу сохраним в parquet\n",
    ").write_parquet(data_path + \"train_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Разбиение train_df на base_model_data и ranker_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = (\n",
    "    pl.scan_parquet(data_path + \"train_df.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Под обучение модели второго уровня выделим 1/5 от данных идущих на train, т.е. ~8 часов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Дата последней интеракиции\n",
    "max_data = train_df.select(\"dt\").max().collect().item()\n",
    "\n",
    "# Дата разделяющая данные для трейна моделей\n",
    "# первого и второго уровней\n",
    "base_ranker_sep = max_data - timedelta(hours=8)\n",
    "\n",
    "# Данные для обучения моделей первого уровня\n",
    "# ranker_data = train_df[(train_df[\"dt\"] >= base_ranker_sep)]\n",
    "# Сразу сохраим в бинарник\n",
    "train_df.filter(pl.col(\"dt\") >= base_ranker_sep).collect().write_parquet(data_path + \"ranker_data.parquet\")\n",
    "\n",
    "# Данные для обучения модели второго уровня\n",
    "# base_models_data = train_df[(train_df[\"dt\"] < base_ranker_sep)]\n",
    "# Сразу сохраим в бинарник\n",
    "train_df.filter(pl.col(\"dt\") < base_ranker_sep).collect().write_parquet(data_path + \"base_models_data.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выделим группы пользователей "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Уникальные айдишники пользователей в таблицах\n",
    "\n",
    "base_users = (\n",
    "    pl.scan_parquet(data_path + \"base_models_data.parquet\")\n",
    "    .select(\"user_id\")\n",
    "    .unique()\n",
    "    .collect()\n",
    ").to_numpy()\n",
    "base_users = base_users.reshape(base_users.shape[0])\n",
    "# save\n",
    "with open(data_path + \"base_users.dill\", \"wb\") as f:\n",
    "    dill.dump(base_users, f)\n",
    "\n",
    "ranker_users = (\n",
    "    pl.scan_parquet(data_path + \"ranker_data.parquet\")\n",
    "    .select(\"user_id\")\n",
    "    .unique()\n",
    "    .collect()\n",
    ").to_numpy()\n",
    "ranker_users = ranker_users.reshape(ranker_users.shape[0])\n",
    "# save\n",
    "with open(data_path + \"ranker_users.dill\", \"wb\") as f:\n",
    "    dill.dump(ranker_users, f)\n",
    "\n",
    "test_users = (\n",
    "    pl.scan_parquet(data_path + \"test_df.parquet\")\n",
    "    .select(\"user_id\")\n",
    "    .unique()\n",
    "    .collect()\n",
    ").to_numpy()\n",
    "test_users = test_users.reshape(test_users.shape[0])\n",
    "# save\n",
    "with open(data_path + \"test_users.dill\", \"wb\") as f:\n",
    "    dill.dump(test_users, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пользователи, которым надо выдавать пресказания для обучения ранкера,\n",
    "# т.е. присутствуют и в base_models_data и в ranker_data (base to ranker users)\n",
    "b2r_users = np.array(list((set(base_users) & set(ranker_users))))\n",
    "display(\"b2r_users\", b2r_users, b2r_users.shape)\n",
    "# save\n",
    "with open(data_path + \"b2r_users.dill\", \"wb\") as f:\n",
    "    dill.dump(b2r_users, f)\n",
    "\n",
    "\n",
    "# на оставшихся пользователях ранкер обучаться не будет\n",
    "# на них просто не будет скоров\n",
    "ranker_only_users = np.array(list(set(ranker_users) - set(base_users)))\n",
    "display(\"ranker_only_users\", ranker_only_users, ranker_only_users.shape)\n",
    "# save\n",
    "with open(data_path + \"ranker_only_users.dill\", \"wb\") as f:\n",
    "    dill.dump(ranker_only_users, f)\n",
    "\n",
    "# Проверим качество на тестовой выборке\n",
    "# Берем только пользователей, которые присутствуют\n",
    "# в base и test выборках\n",
    "b2t_users = np.array(list(set(test_users) & (set(base_users))))\n",
    "display(\"b2t_users\", b2t_users, b2t_users.shape)\n",
    "with open(data_path + \"b2t_users.dill\", \"wb\") as f:\n",
    "    dill.dump(b2t_users, f)\n",
    "\n",
    "# Пользователи из test_df, которым будут выданы\n",
    "# таргетирвонные рекомондации\n",
    "bNr2t_users = np.array(list((set(base_users) | set(ranker_users)) & set(test_users)))\n",
    "display(\"bNr2t_users\", bNr2t_users, bNr2t_users.shape)\n",
    "# save\n",
    "with open(data_path + \"bNr2t_users.dill\", \"wb\") as f:\n",
    "    dill.dump(bNr2t_users, f)\n",
    "\n",
    "\n",
    "# Пользователи, которые присутствуют только в test_df (cold_users)\n",
    "test_only_users = np.array(list(set(test_users) - (set(base_users) | set(ranker_users))))\n",
    "display(\"test_only_users\", test_only_users, test_only_users.shape)\n",
    "# save\n",
    "with open(data_path + \"test_only_users.dill\", \"wb\") as f:\n",
    "    dill.dump(test_only_users, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
