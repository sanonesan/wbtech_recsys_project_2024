"""
Module for preprocessing data for ranker
"""

from typing import Optional, Union

# ----------------
# Data processing
# ----------------
import dill

import numpy as np
import polars as pl

from sklearn.model_selection import train_test_split

from src.ml.data.feature_utils import ITEM_CATEGORIAL_FEATURES, ITEM_NUMERICAL_FEATURES

from src.logs.console_logger import LOGGER


RANDOM_STATE = 42


def __merge_candidates_from_1_stage(candidates_data_path):
    """
    Merges candidate recommendations from different first-stage models.

    This function reads candidate recommendations generated by different KNN and LFM models,
    joins them into a single table, and handles missing values by filling with defaults.
    The merged candidates are saved to a single parquet file.

    Args:
        candidates_data_path (str): Path to the directory containing the candidates data files.
    """

    default_values_merged = {
        "cos_score": pl.col("cos_score").min() - 0.01,
        "bm25_score": pl.col("bm25_score").min() - 0.01,
        "tfidf_score": pl.col("tfidf_score").min() - 0.01,
        "lfm_score": pl.col("lfm_score").min() - 0.01,
        "cos_rank": pl.col("cos_rank").max() + 1,
        "bm25_rank": pl.col("bm25_rank").max() + 1,
        "tfidf_rank": pl.col("tfidf_rank").max() + 1,
        "lfm_rank": pl.col("lfm_rank").max() + 1,
    }

    candidates_list = ["cos", "bm25", "tfidf", "lfm"]

    candidates = pl.scan_parquet(
        candidates_data_path + f"candidates_{candidates_list[0]}.parquet"
    ).filter(pl.col(f"{candidates_list[0]}_rank") < 15)

    for cand in candidates_list[1:]:
        candidates = (
            candidates.join(
                other=pl.scan_parquet(
                    candidates_data_path + f"candidates_{cand}.parquet"
                ).filter(pl.col(f"{cand}_rank") < 15),
                how="outer",
                on=["user_id", "item_id"],
            )
            .with_columns(
                pl.col("user_id").fill_null(pl.col("user_id_right")),
                pl.col("item_id").fill_null(pl.col("item_id_right")),
            )
            .drop(["user_id_right", "item_id_right"])
        )

    candidates.collect().with_columns(
        (
            pl.col(col_name).fill_null(default_values_merged[col_name])
            for col_name in default_values_merged.keys()
        )
    ).write_parquet(candidates_data_path + "candidates_full.parquet")


def __func_feature_engineering_with_interactions(
    df: Union[pl.LazyFrame, pl.DataFrame],
) -> Union[pl.LazyFrame, pl.DataFrame]:
    """
    Calculates new interaction-based features for users and items.

    This function calculates features such as item popularity, user average item popularity,
    item average user history length, and user last viewed item popularity based on user-item interactions.

    Args:
        df (Union[pl.LazyFrame, pl.DataFrame]): Input DataFrame or LazyFrame with user-item interaction data.
        It should have columns `user_id`, `item_id`, `dt` and `u_total_inter` (renamed to `user_hist`).

    Returns:
        Union[pl.LazyFrame, pl.DataFrame]: DataFrame or LazyFrame with the new features.
    """
    # переименуем для удобства
    df = df.rename(
        {
            "u_total_inter": "user_hist",
        }
    )
    # Получаем популярность контента
    df = df.with_columns(pl.col("user_id").count().over("item_id").alias("item_pop"))

    # Получаем среднюю популярность контента, просматриваемого этим юзером
    df = df.with_columns(
        pl.col("item_pop").mean().over("user_id").alias("user_avg_pop")
    )

    # Получаем среднюю длину истории пользователя, которые смотрит этот контент
    df = df.with_columns(
        pl.col("user_hist").mean().over("item_id").alias("item_avg_hist")
    )

    # Получаем популярность последнего просмотренного контента
    df = df.sort(
        by=["user_id", "dt"],
        descending=[False, True],
    )
    df = df.with_columns(
        pl.col("item_pop").first().over("user_id").alias("user_last_pop")
    )

    return df


def __get_tables_with_users_and_items_features(
    interactions_path: str,
    users_path: Optional[str] = None,
    items_path: Optional[str] = None,
    save_users_path: Optional[str] = None,
    save_items_path: Optional[str] = None,
):
    """
    Processes interactions, users, and items data to create feature-enriched tables.

    This function reads interaction data along with optional user and item feature data,
    calculates features, and optionally saves the processed user and item tables to parquet files.

    Args:
        interactions_path (str): Path to the parquet file containing interactions data.
        users_path (Optional[str], optional): Path to the parquet file containing user features.
        items_path (Optional[str], optional): Path to the parquet file containing item features.
        save_users_path (Optional[str], optional): Path where the processed user table is saved.
        save_items_path (Optional[str], optional): Path where the processed item table is saved.
    """
    interactions = pl.scan_parquet(interactions_path)

    if users_path:
        df_users = pl.scan_parquet(users_path)
    elif save_users_path:
        df_users = interactions.select("user_id").unique()
    else:
        raise "users_path or save_users_path should be passed to funciton"

    if items_path:
        df_items = pl.scan_parquet(items_path)
    elif save_items_path:
        df_items = interactions.select("item_id").unique()
    else:
        raise "users_path or save_users_path should be passed to funciton"

    # Добавляем новые фичи в соответствующие таблицы
    df_items.join(
        other=__func_feature_engineering_with_interactions(interactions)
        .select(["item_id", "item_pop", "item_avg_hist"])
        .unique(),
        how="left",
        on="item_id",
    ).fill_null(0).collect().write_parquet(save_items_path)

    # Создаем таблицу с фитчами пользователей
    df_users.join(
        other=__func_feature_engineering_with_interactions(interactions)
        .select(["user_id", "user_hist", "user_avg_pop", "user_last_pop"])
        .unique(),
        how="left",
        on="user_id",
    ).fill_null(0).collect().write_parquet(save_users_path)


def __users_filter(
    user_list: np.ndarray,
    candidates_df: pl.LazyFrame,
    df: pl.LazyFrame,
    default_values_candidates: dict,
) -> pl.DataFrame:
    """
    Filters user interaction data and candidate recommendations,
    ensuring each user has both interactions and recommendations.

    Args:
        user_list (np.ndarray): User IDs to include.
        candidates_df (pl.LazyFrame): Candidate item recommendations
            with ranks ('cos_rank', 'bm25_rank', 'lfm_rank', 'tfidf_rank').
        df (pl.LazyFrame): User-item interactions ('user_id', 'item_id', 'dt',
            and potentially other weight-based columns).

    Returns:
        pl.LazyFrame: Filtered and merged DataFrame with user interactions
            and candidate items sorted and with missing values filled.
            It also filters down to items with at least one rank < 15
    """
    # For fillna
    default_values = {
        "ui_inter": 0,
        **default_values_candidates,
    }

    # Get valid interactions
    df = df.filter(pl.col("user_id").is_in(user_list))
    candidates_df = candidates_df.filter(pl.col("user_id").is_in(user_list))

    # join interaction на наших кандидатов для users из train, val, test
    df = (
        df.join(
            other=candidates_df,
            how="outer",
            on=["user_id", "item_id"],
        )
        .with_columns(
            pl.col("user_id").fill_null(pl.col("user_id_right")),
            pl.col("item_id").fill_null(pl.col("item_id_right")),
        )
        .drop(["user_id_right", "item_id_right"])
    )
    df = df.collect().with_columns(
        (
            pl.col(col_name).fill_null(default_values[col_name])
            for col_name in default_values.keys()
        )
    )

    # Сортируем по user_id
    df = df.sort(by=["user_id", "item_id"])
    df = df.filter(
        (pl.col("cos_rank") < 15)
        | (pl.col("bm25_rank") < 15)
        | (pl.col("lfm_rank") < 15)
        | (pl.col("tfidf_rank") < 15)
    )

    return df


# Добавляем фичи
def __add_users_features(
    df: pl.LazyFrame,
    users: pl.LazyFrame,
    default_values_users: dict,
) -> pl.DataFrame:
    """
    Merges user and item features into a DataFrame, handling missing values.

    Args:
        df (pl.LazyFrame): Interaction DataFrame ('user_id', 'item_id').
        users (pl.LazyFrame): User features DataFrame ('user_id').

    Returns:
        pl.DataFrame: DataFrame with merged user and item features,
            and missing values filled.
    """

    df = df.join(
        other=users.filter(
            pl.col("user_id").is_in(df.select("user_id").unique().collect())
        ),
        how="left",
        on=["user_id"],
    )

    # При джойне могут получиться строки
    # с несуществующими айтемами или юзерами.
    # Заполняем пропуски
    return df.collect().with_columns(
        (
            pl.col(col_name).fill_null(default_values_users[col_name])
            for col_name in default_values_users.keys()
        )
    )


# Добавляем фичи
def __add_items_features(
    df: pl.LazyFrame,
    items: pl.LazyFrame,
    default_values_items: dict,
) -> pl.DataFrame:
    """
    Merges user and item features into a DataFrame, handling missing values.

    Args:
        df (pd.DataFrame): Interaction DataFrame ('user_id', 'item_id').
        items (pd.DataFrame): Item features DataFrame ('item_id').

    Returns:
        pd.DataFrame: DataFrame with merged user and item features,
            and missing values filled.
    """

    df = df.join(
        other=items.filter(
            pl.col("item_id").is_in(df.select("item_id").unique().collect())
        ),
        how="left",
        on=["item_id"],
    )

    # При джойне могут получиться строки
    # с несуществующими айтемами или юзерами.
    # Заполняем пропуски
    return df.collect().with_columns(
        (
            pl.col(col_name).fill_null(default_values_items[col_name])
            for col_name in default_values_items.keys()
        )
    )


def __add_target(df: pl.LazyFrame) -> pl.DataFrame:
    """
    Adds a target column to a DataFrame based on interaction counts.

    This function takes a Polars LazyFrame or DataFrame, checks the number of interactions
    represented by the `ui_inter` column, and assigns a numerical target value based on predefined thresholds.

    Args:
        df (pl.LazyFrame): Input LazyFrame or DataFrame.
                Must have the column `ui_inter`.

    Returns:
        pl.DataFrame: DataFrame with the newly added `target` column.
    """
    return df.with_columns(
        pl.when(pl.col("ui_inter") > 6)
        .then(10)
        .when(pl.col("ui_inter") > 4)
        .then(8)
        .when(pl.col("ui_inter") > 2)
        .then(4)
        .when(pl.col("ui_inter") > 1)
        .then(2)
        .otherwise(1)
        .alias("target")
    ).collect()


def preprocess_data_for_ranker_trainning(
    data_path,
    candidates_data_path,
    do_ranker_test: bool = False,
):
    """
    Preprocesses data for ranker training.

    Args:
        data_path (str): Path to the directory containing the base data files.
        candidates_data_path (str): Path to the directory containing candidate data.
        do_ranker_test (bool, optional): If True, generates a ranker_test set. Defaults to False.
    """
    try:
        LOGGER.info(msg="Merging candidates: started...")
        __merge_candidates_from_1_stage(candidates_data_path)
        LOGGER.info(msg="Merging candidates: finished!")

        LOGGER.info(msg="Recalculating df_items table: started...")
        try:
            # При перепрогоне колонки необходимо перерасчитать
            pl.scan_parquet(data_path + "df_items.parquet").drop(
                ["item_pop", "item_avg_hist"]
            ).collect().write_parquet(data_path + "df_items.parquet")
        except:
            pass
        LOGGER.info(msg="Recalculating df_items table: finished!")

        LOGGER.info(msg="Get tables with users and items features: started...")
        __get_tables_with_users_and_items_features(
            interactions_path=data_path + "base_models_data.parquet",
            users_path=None,
            items_path=data_path + "df_items.parquet",
            save_users_path=data_path + "df_users.parquet",
            save_items_path=data_path + "df_items.parquet",
        )
        LOGGER.info(msg="Get tables with users and items features: finished!")

        LOGGER.info(msg="Reading tables and users: started...")
        ranker_data = pl.scan_parquet(data_path + "ranker_data.parquet")
        candidates_full = pl.scan_parquet(
            candidates_data_path + "candidates_full.parquet"
        )

        with (
            # Пользователи, которым надо выдавать пресказания для обучения ранкера,
            # т.е. присутствуют и в base_models_data и в ranker_data (base to ranker users)
            open(data_path + "b2r_users.dill", "rb") as f_b2r,
            # Пользователи из test_df, которым будут выданы
            # таргетирвонные рекомондации
            open(data_path + "bNr2t_users.dill", "rb") as fbNr2t,
        ):
            b2r_users = dill.load(f_b2r)
            bNr2t_users = dill.load(fbNr2t)

        LOGGER.info(msg="Reading tables and users: finished!")

        LOGGER.info(msg="Set default candidates values: started...")
        # Set default candidates values
        candidates_list = ["cos", "bm25", "tfidf", "lfm"]
        default_values_candidates = {}
        for cand in candidates_list:
            default_values_candidates[f"{cand}_score"] = (
                candidates_full.select(f"{cand}_score").min().collect().item()
            )
            default_values_candidates[f"{cand}_rank"] = (
                candidates_full.select(f"{cand}_rank").max().collect().item()
            )
        LOGGER.info(msg="Set default candidates values: finished!")

        LOGGER.info(msg="Filter data for trainning: started...")
        # Оставим только необходимые параметры из таблицы

        # Ранкер будем обучать на пользователях у кого длинная история взаимодействий
        ranker_data = ranker_data.filter(pl.col("u_total_inter") > 75).select(
            [
                "user_id",
                "item_id",
                # Так как бьем данные для tain val не по времени,
                # колонка "dt" не нужна
                # --------------------------
                # Потом будем использовать для ранкера чтобы задать таргет
                # (количество взаимодействий с предметом)
                "ui_inter",
                # Веса убираем, т.к. они были получены из схожих соображений
                # и зависят от +- одинаковых фитчей
                # Остальные колонки не нужны
                # Так как они были использованы для вывода весовых колонок,
                # либо присутствуют в фитчах пользователя или айтема
            ]
        )
        LOGGER.info(msg="Filter data for trainning: finished!")

        LOGGER.info(msg="Train \\ Val \\ Test Split: started...")
        # ------------------------
        # Train \ Val \ Test Split
        # ------------------------

        # Теперь ranker_data разбиваем по юзерам
        # на train и val для обучения и валидации ранкера
        val_size = 0.2

        ranker_train_users, ranker_val_users = train_test_split(
            ranker_data.select("user_id")
            .filter(pl.col("user_id").is_in(b2r_users))
            .collect()
            .to_numpy()
            .flatten(),
            random_state=RANDOM_STATE,
            test_size=val_size,
        )

        # test-выборка у нас уже имеется
        # выборка пользователей присутствующих в base & ranker & test
        # на них и будем проводить первичный тест системы
        ranker_test_users = bNr2t_users

        LOGGER.info(msg="Train \\ Val \\ Test Split: finished!")

        LOGGER.info(msg="Filter users from ranker tables: started...")
        ranker_users_sets_dict = {
            "ranker_train": ranker_train_users,
            "ranker_val": ranker_val_users,
            "ranker_test": ranker_test_users,
        }

        # Оставляем среди users только тех, для кого есть
        # и рекомендации и таргеты

        for set_name, set_users in ranker_users_sets_dict.items():
            if (set_name == "ranker_test") & (not do_ranker_test):
                continue

            __users_filter(
                user_list=set_users,
                candidates_df=candidates_full,
                df=ranker_data,
                default_values_candidates=default_values_candidates,
            ).write_parquet(data_path + f"{set_name}.parquet")

        LOGGER.info(msg="Filter users from ranker tables: finished!")

        LOGGER.info(msg="Adding users features: started...")
        # Загружаем таблицу фитчей пользователей
        df_users = pl.scan_parquet(data_path + "df_users.parquet")

        # Для новых фичей юзеров
        default_values_users = {
            "user_hist": 0,
            "user_avg_pop": df_users.select("user_avg_pop").median().collect().item(),
            "user_last_pop": df_users.select("user_last_pop").median().collect().item(),
        }

        # Добавляем фичи юзеров

        for set_name, set_users in ranker_users_sets_dict.items():
            if (set_name == "ranker_test") & (not do_ranker_test):
                continue

            __add_users_features(
                pl.scan_parquet(data_path + f"{set_name}.parquet"),
                df_users,
                default_values_users,
            ).write_parquet(data_path + f"{set_name}.parquet")

        LOGGER.info(msg="Adding users features: finished!")

        LOGGER.info(msg="Adding items features: started...")

        # Загружаем таблицу айтемов
        df_items = pl.scan_parquet(data_path + "df_items.parquet")

        # Для новых фичей айтемов
        default_values_items = {}

        for num in ITEM_NUMERICAL_FEATURES:
            default_values_items[num] = df_items.select(num).median().collect().item()

        for cat in ITEM_CATEGORIAL_FEATURES:
            default_values_items[cat] = (
                df_items.group_by(cat)
                .agg(pl.col(cat).count().alias("count"))
                .sort("count", descending=True)
                .select(cat)
                .first()
                .collect()
                .item()
            )

        # Добавляем фичи айтемов

        for set_name, set_users in ranker_users_sets_dict.items():
            if (set_name == "ranker_test") & (not do_ranker_test):
                continue

            __add_items_features(
                pl.scan_parquet(data_path + f"{set_name}.parquet"),
                df_items,
                default_values_items,
            ).write_parquet(data_path + f"{set_name}.parquet")

        LOGGER.info(msg="Adding items features: finished!")

        LOGGER.info(msg="Adding target: started...")

        # Добавим таргет

        for set_name, set_users in ranker_users_sets_dict.items():
            if (set_name == "ranker_test") & (not do_ranker_test):
                continue

            __add_target(
                pl.scan_parquet(data_path + f"{set_name}.parquet")
            ).write_parquet(data_path + f"{set_name}.parquet")
        LOGGER.info(msg="Adding target: finished!")

    except Exception as e:
        LOGGER.error(
            msg=f"Error occured while preprocessing data for ranker inference: {e}!"
        )
        raise e


def preprocess_data_for_ranker_inference(data_path, candidates_data_path):
    """
    Preprocesses data for ranker inference.

    Args:
        data_path (str): Path to the directory containing the base data files.
        candidates_data_path (str): Path to the directory containing candidate data.
    """

    try:
        LOGGER.info(msg="Merging candidates: started...")

        __merge_candidates_from_1_stage(candidates_data_path)

        LOGGER.info(msg="Merging candidates: finished!")

        LOGGER.info(msg="Recalculating df_items table: started...")
        try:
            # Так как импортируем таблицу с этапа 2.1:
            # в ней рассчитаны значения
            # для колонок "item_pop", "item_avg_hist"
            # Сейчас данные колонки необходимо перерасчитать
            pl.scan_parquet(data_path + "df_items.parquet").drop(
                ["item_pop", "item_avg_hist"]
            ).collect().write_parquet(data_path + "df_items.parquet")
        except:
            pass

        LOGGER.info(msg="Recalculating df_items table: finished!")

        LOGGER.info(
            msg="Concat interactions datasets from base & ranker stages: started..."
        )
        # Создадим датасет взаимодействий `ranker_data_bNr`
        pl.concat(
            [
                pl.scan_parquet(data_path + "base_models_data.parquet").select(
                    ["user_id", "item_id", "dt", "ui_inter", "u_total_inter"]
                ),
                pl.scan_parquet(data_path + "ranker_data.parquet").select(
                    ["user_id", "item_id", "dt", "ui_inter", "u_total_inter"]
                ),
            ],
            how="vertical",
        ).collect().write_parquet(data_path + "ranker_data_bNr.parquet")
        LOGGER.info(
            msg="Concat interactions datasets from base & ranker stages: finished!"
        )

        LOGGER.info(msg="Get tables with users and items features: started...")
        __get_tables_with_users_and_items_features(
            interactions_path=data_path + "ranker_data_bNr.parquet",
            users_path=None,
            items_path=data_path + "df_items.parquet",
            save_users_path=data_path + "df_users.parquet",
            save_items_path=data_path + "df_items.parquet",
        )
        LOGGER.info(msg="Get tables with users and items features: finished!")

        LOGGER.info(msg="Reading tables and users: started...")

        with (
            # Пользователи из test_df, которым будут выданы
            # таргетирвонные рекомондации
            open(data_path + "bNr2t_users.dill", "rb") as users_f,
        ):
            bNr2t_users = dill.load(users_f)

        ranker_data = (
            pl.scan_parquet(data_path + "ranker_data_bNr.parquet")
            .select(["user_id", "item_id", "ui_inter"])
            .filter(pl.col("user_id").is_in(bNr2t_users))
        )
        candidates_full = pl.scan_parquet(
            candidates_data_path + "candidates_full.parquet"
        ).filter(pl.col("user_id").is_in(bNr2t_users))

        LOGGER.info(msg="Reading tables and users: finished!")

        LOGGER.info(msg="Set default candidates values: started...")
        # Set default candidates values
        candidates_list = ["cos", "bm25", "tfidf", "lfm"]
        default_values_candidates = {}
        for cand in candidates_list:
            default_values_candidates[f"{cand}_score"] = (
                candidates_full.select(f"{cand}_score").min().collect().item()
            )
            default_values_candidates[f"{cand}_rank"] = (
                candidates_full.select(f"{cand}_rank").max().collect().item()
            )
        LOGGER.info(msg="Set default candidates values: finished!")

        LOGGER.info(msg="Filter users from ranker tables: started!")

        # Оставляем среди users только тех, для кого есть
        # и рекомендации и таргеты

        __users_filter(
            user_list=bNr2t_users,
            candidates_df=candidates_full,
            df=ranker_data,
            default_values_candidates=default_values_candidates,
        ).write_parquet(data_path + "ranker_data_bNr.parquet")

        LOGGER.info(msg="Filter users from ranker tables: finished!")

        LOGGER.info(msg="Adding users features: started...")
        # Загружаем таблицу фитчей пользователей
        df_users = pl.scan_parquet(data_path + "df_users.parquet")

        # Для новых фичей юзеров
        default_values_users = {
            "user_hist": 0,
            "user_avg_pop": df_users.select("user_avg_pop").median().collect().item(),
            "user_last_pop": df_users.select("user_last_pop").median().collect().item(),
        }

        # Добавляем фичи юзеров
        __add_users_features(
            pl.scan_parquet(data_path + "ranker_data_bNr.parquet"),
            df_users,
            default_values_users,
        ).write_parquet(data_path + "ranker_data_bNr.parquet")

        LOGGER.info(msg="Adding items features: finished!")

        # Загружаем таблицу айтемов
        df_items = pl.scan_parquet(data_path + "df_items.parquet")

        LOGGER.info(msg="Adding items features: started...")
        # Для новых фичей айтемов
        default_values_items = {}

        for num in ITEM_NUMERICAL_FEATURES:
            default_values_items[num] = df_items.select(num).median().collect().item()

        for cat in ITEM_CATEGORIAL_FEATURES:
            default_values_items[cat] = (
                df_items.group_by(cat)
                .agg(pl.col(cat).count().alias("count"))
                .sort("count", descending=True)
                .select(cat)
                .first()
                .collect()
                .item()
            )

        # Добавляем фичи айтемов
        __add_items_features(
            pl.scan_parquet(data_path + "ranker_data_bNr.parquet"),
            df_items,
            default_values_items,
        ).write_parquet(data_path + "ranker_data_bNr.parquet")
        LOGGER.info(msg="Adding items features: finished!")

    except Exception as e:
        LOGGER.error(
            msg=f"Error occured while preprocessing data for ranker inference: {e}!"
        )
        raise e
