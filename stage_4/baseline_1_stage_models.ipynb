{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WB RecSys Project\n",
    "\n",
    "# Общее описание проекта\n",
    "\n",
    "Необходимо на основании взаимодействий пользователей с товарами предсказать следующие взаимодействия пользователей с товарами.\n",
    "\n",
    "# Stage 4\n",
    "\n",
    "- Выбрать метрику оценки качества и обосновать выбор\n",
    "- Разработать baseline (может быть несколько алгоритмов)\n",
    "- Реализовать выбранное решение/я\n",
    "- Протестировать работу baseline\n",
    "- Выбрать итоговое решение для дальнейшей оптимизации и обосновать выбор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "# ----------------\n",
    "# Data processing\n",
    "# ----------------\n",
    "import dill\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "# RecSys models imports\n",
    "# ---------------------\n",
    "from lightfm import LightFM\n",
    "\n",
    "from rectools import Columns\n",
    "from rectools.dataset import Dataset as RTDataset\n",
    "from rectools.models import (\n",
    "    PopularModel,\n",
    "    LightFMWrapperModel,\n",
    "    implicit_knn,\n",
    ")\n",
    "from implicit import nearest_neighbours\n",
    "from mab2rec import BanditRecommender, LearningPolicy\n",
    "\n",
    "\n",
    "# --------------\n",
    "# Plotting libs\n",
    "# --------------\n",
    "# import plotly.express as px\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import shap\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Metrics Evaluation\n",
    "# -------------------\n",
    "from metrics import RecommenderMetrics\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data_closed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Загружаем таблицу айтемов\n",
    "# with open(data_path + \"df_items.dill\", \"rb\") as f:\n",
    "#     df_items = dill.load(f)\n",
    "\n",
    "# Загружаем таблицу данных для моделей первого уровня\n",
    "with open(data_path + \"base_models_data.dill\", \"rb\") as f:\n",
    "    base_models_data = dill.load(f)\n",
    "\n",
    "# Загружаем таблицу данных для ранкера\n",
    "with open(data_path + \"ranker_data.dill\", \"rb\") as f:\n",
    "    ranker_data = dill.load(f)\n",
    "\n",
    "# Загружаем таблицу тестовых данных\n",
    "with open(data_path + \"test_df.dill\", \"rb\") as f:\n",
    "    test_df = dill.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним структуру данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df_items\")\n",
    "display(df_items.dtypes)\n",
    "\n",
    "\n",
    "print(\"\\nbase_models_data\")\n",
    "display(base_models_data.dtypes)\n",
    "\n",
    "\n",
    "print(\"\\nranker_data\")\n",
    "display(ranker_data.dtypes)\n",
    "\n",
    "\n",
    "print(\"\\ntest_df\")\n",
    "display(test_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выборки по пользователям (USER_ID), участвующим в разных этапах обучения моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Уникальные айдишники пользователей в таблицах\n",
    "base_users = base_models_data[\"user_id\"].unique()\n",
    "# save\n",
    "with open(data_path + \"base_users.dill\", \"wb\") as f:\n",
    "    dill.dump(base_users, f)\n",
    "\n",
    "ranker_users = ranker_data[\"user_id\"].unique()\n",
    "# save\n",
    "with open(data_path + \"ranker_users.dill\", \"wb\") as f:\n",
    "    dill.dump(ranker_users, f)\n",
    "\n",
    "test_users = test_df[\"user_id\"].unique()\n",
    "# save\n",
    "with open(data_path + \"test_users.dill\", \"wb\") as f:\n",
    "    dill.dump(test_users, f)\n",
    "\n",
    "# Пользователи, которым надо выдавать пресказания для обучения ранкера,\n",
    "# т.е. присутствуют и в base_models_data и в ranker_data (base to ranker users)\n",
    "b2r_users = np.array(list((set(base_users) & set(ranker_users))))\n",
    "display(\"b2r_users\", b2r_users, b2r_users.shape)\n",
    "# save\n",
    "with open(data_path + \"b2r_users.dill\", \"wb\") as f:\n",
    "    dill.dump(b2r_users, f)\n",
    "\n",
    "\n",
    "# на оставшихся пользователях ранкер обучаться не будет\n",
    "# на них просто не будет скоров\n",
    "ranker_only_users = np.array(list(set(ranker_users) - set(base_users)))\n",
    "display(\"ranker_only_users\", ranker_only_users, ranker_only_users.shape)\n",
    "# save\n",
    "with open(data_path + \"ranker_only_users.dill\", \"wb\") as f:\n",
    "    dill.dump(ranker_only_users, f)\n",
    "\n",
    "\n",
    "# Пользователи из test_df, которым будут выданы\n",
    "# таргетирвонные рекомондации\n",
    "bNr2t_users = np.array(list((set(base_users) | set(ranker_users)) & set(test_users)))\n",
    "display(\"bNr2t_users\", bNr2t_users, bNr2t_users.shape)\n",
    "# save\n",
    "with open(data_path + \"bNr2t_users.dill\", \"wb\") as f:\n",
    "    dill.dump(bNr2t_users, f)\n",
    "\n",
    "# Пользователи, которые присутствуют только в test_df (cold_users)\n",
    "test_only_users = np.array(list(set(test_users) - (set(base_users) | set(ranker_users))))\n",
    "display(\"test_only_users\", test_only_users, test_only_users.shape)\n",
    "# save\n",
    "with open(data_path + \"test_only_users.dill\", \"wb\") as f:\n",
    "    dill.dump(test_only_users, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение моделей первого уровня для обучения ранкера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = \"../models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели максимально простые, основанные на взаимодействиях пользователей и айтемов.\n",
    "\n",
    "Фитчи айтемов оставим для переранжирования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Качество работы моделей будем оценивать следующим образом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверим качество на тестовой выборке\n",
    "# Берем только пользователей, которые присутствуют\n",
    "# в base и test выборках\n",
    "b2t_users = np.array(list(set(test_users) & (set(base_users))))\n",
    "b2t_users, b2t_users.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пользователей много, так что выберем\n",
    "# 100 тысяч пользователей, на которых расчитаем метрики\n",
    "b2t_users = np.random.choice(\n",
    "    b2t_users,\n",
    "    size=10**5,\n",
    "    replace=False,\n",
    ")\n",
    "b2t_users, b2t_users.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим часть таблицы, на которой будем сверяться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_tmp= test_df[test_df[\"user_id\"].isin(b2t_users)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectools Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используется библиотека `rectools`, так что dataset должен содержать 4 основные колонки: \n",
    "- user_id,\n",
    "- item_id,\n",
    "- datetime,\n",
    "- weight,\n",
    "\n",
    "где weight &mdash; максимально хорошо описывает вес предмета (важность взаимодейстия) в момент взаимодействия с ним пользователем. \n",
    "\n",
    "Из таблицы взаимодействий такими параметрами являются:\n",
    "- `weight`: описывает общий вес взаимодействия (кол-во взаимодействий с предметом / общее число взаимодействий), \n",
    "- `cum_weight`: вес предмета в зависимости от номера вхождения этого предмета (отношение номера входжения предмета к общему числу взаимодействий с предметом, умноженное на рейтинг этого предмета), \n",
    "- `rel_weight`: вес соответствующий товару при каждом новом взаимодействии пользователя с товаром (отношения числа взаимодейсвий с предметом к общему число взаимодействий со всеми предметами на момент записи взаимодействия)\n",
    "\n",
    "В контекстке учета времени для получения веса интеракции, на мой взгляд, самым валидным параметром является `rel_weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Изменим датасет `base_models_data`\n",
    "# Оставим только нужные колонки и переименуем под стандарт `rectools`\n",
    "base_models_data = base_models_data[\n",
    "    [\n",
    "        \"user_id\",\n",
    "        \"item_id\",\n",
    "        \"dt\",\n",
    "        \"rel_weight\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "base_models_data = base_models_data.rename(\n",
    "    columns={\n",
    "        \"user_id\": Columns.User,\n",
    "        \"item_id\": Columns.Item,\n",
    "        \"dt\": Columns.Datetime,\n",
    "        \"rel_weight\": Columns.Weight,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Создадим датасет взаимодействий\n",
    "current_dataset = RTDataset.construct(\n",
    "    interactions_df=base_models_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectools PopularModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_model = PopularModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_model.fit(current_dataset)\n",
    "\n",
    "%clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "with open(models_path + \"popular_model.dill\", \"wb\") as f:\n",
    "    dill.dump(popular_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "with open(models_path + \"popular_model.dill\", \"rb\") as f:\n",
    "    popular_model = dill.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test PopularModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_pop = popular_model.recommend(\n",
    "    b2t_users,\n",
    "    current_dataset,\n",
    "    # выдаем 10 кандидатов\n",
    "    k=10,\n",
    "    # рекомендуем уже просмотренные товары\n",
    "    filter_viewed=False,\n",
    ")\n",
    "candidates_pop = candidates_pop.rename(\n",
    "    columns={\n",
    "        \"score\": \"pop_score\",\n",
    "        \"rank\": \"pop_rank\",\n",
    "    }\n",
    ")\n",
    "candidates_pop.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (\n",
    "    candidates_pop[candidates_pop[\"pop_rank\"] <= 10][[\"user_id\", \"item_id\"]]\n",
    "    .groupby(by=\"user_id\")[\"item_id\"]\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"item_id\": \"pop_recs\"})\n",
    ")\n",
    "\n",
    "metrics_df_tmp = pd.merge(metrics_df_tmp, predictions, how=\"left\", on=\"user_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RecommenderMetrics.evaluate_recommender(\n",
    "    metrics_df_tmp,\n",
    "    model_preds_col=\"pop_recs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectools Implicit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_impl_cos_k50 = implicit_knn.ImplicitItemKNNWrapperModel(\n",
    "    model=nearest_neighbours.CosineRecommender(K=50)\n",
    ")\n",
    "\n",
    "knn_impl_bm25_k50 = implicit_knn.ImplicitItemKNNWrapperModel(\n",
    "    model=nearest_neighbours.BM25Recommender(K=50)\n",
    ")\n",
    "\n",
    "knn_impl_tfidf_k50 = implicit_knn.ImplicitItemKNNWrapperModel(\n",
    "    model=nearest_neighbours.TFIDFRecommender(K=50)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models\n",
    "knn_impl_cos_k50.fit(current_dataset)\n",
    "knn_impl_bm25_k50.fit(current_dataset)\n",
    "knn_impl_tfidf_k50.fit(current_dataset)\n",
    "\n",
    "%clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "\n",
    "with open(models_path + \"knn_impl_cos_k50.dill\", \"wb\") as f:\n",
    "    dill.dump(knn_impl_cos_k50, f)\n",
    "\n",
    "with open(models_path + \"knn_impl_bm25_k50.dill\", \"wb\") as f:\n",
    "    dill.dump(knn_impl_bm25_k50, f)\n",
    "\n",
    "with open(models_path + \"knn_impl_tfidf_k50.dill\", \"wb\") as f:\n",
    "    dill.dump(knn_impl_tfidf_k50, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "\n",
    "with open(models_path + \"knn_impl_cos_k50.dill\", \"rb\") as f:\n",
    "    knn_impl_cos_k50 = dill.load(f)\n",
    "\n",
    "with open(models_path + \"knn_impl_bm25_k50.dill\", \"rb\") as f:\n",
    "    knn_impl_bm25_k50 = dill.load(f)\n",
    "\n",
    "with open(models_path + \"knn_impl_tfidf_k50.dill\", \"rb\") as f:\n",
    "    knn_impl_tfidf_k50 = dill.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_cos = knn_impl_cos_k50.recommend(\n",
    "    b2t_users,\n",
    "    current_dataset,\n",
    "    # выдаем 10 кандидатов\n",
    "    k=10,\n",
    "    # рекомендуем уже просмотренные товары\n",
    "    filter_viewed=False,\n",
    ")\n",
    "candidates_cos = candidates_cos.rename(\n",
    "    columns={\n",
    "        \"score\": \"cos_score\",\n",
    "        \"rank\": \"cos_rank\",\n",
    "    }\n",
    ")\n",
    "candidates_cos.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (\n",
    "    candidates_cos[candidates_cos[\"cos_rank\"] <= 10][[\"user_id\", \"item_id\"]]\n",
    "    .groupby(by=\"user_id\")[\"item_id\"]\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"item_id\": \"cos_recs\"})\n",
    ")\n",
    "metrics_df_tmp = pd.merge(metrics_df_tmp, predictions, how=\"left\", on=\"user_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RecommenderMetrics.evaluate_recommender(\n",
    "    metrics_df_tmp,\n",
    "    model_preds_col=\"cos_recs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_bm25 = knn_impl_bm25_k50.recommend(\n",
    "    b2t_users,\n",
    "    current_dataset,\n",
    "    # выдаем 10 кандидатов\n",
    "    k=10,\n",
    "    # рекомендуем уже просмотренные товары\n",
    "    filter_viewed=False,\n",
    ")\n",
    "candidates_bm25 = candidates_bm25.rename(\n",
    "    columns={\n",
    "        \"score\": \"bm25_score\",\n",
    "        \"rank\": \"bm25_rank\",\n",
    "    }\n",
    ")\n",
    "candidates_bm25.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (\n",
    "    candidates_bm25[candidates_bm25[\"bm25_rank\"] <= 10][[\"user_id\", \"item_id\"]]\n",
    "    .groupby(by=\"user_id\")[\"item_id\"]\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"item_id\": \"bm25_recs\"})\n",
    ")\n",
    "\n",
    "metrics_df_tmp = pd.merge(\n",
    "    metrics_df_tmp,\n",
    "    predictions,\n",
    "    how=\"left\",\n",
    "    on=\"user_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RecommenderMetrics.evaluate_recommender(\n",
    "    metrics_df_tmp,\n",
    "    model_preds_col=\"bm25_recs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_tfidf = knn_impl_tfidf_k50.recommend(\n",
    "    b2t_users,\n",
    "    current_dataset,\n",
    "    # выдаем 10 кандидатов\n",
    "    k=10,\n",
    "    # рекомендуем уже просмотренные товары\n",
    "    filter_viewed=False,\n",
    ")\n",
    "candidates_tfidf = candidates_tfidf.rename(\n",
    "    columns={\n",
    "        \"score\": \"tfidf_score\",\n",
    "        \"rank\": \"tfidf_rank\",\n",
    "    }\n",
    ")\n",
    "candidates_tfidf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (\n",
    "    candidates_tfidf[candidates_tfidf[\"tfidf_rank\"] <= 10][[\"user_id\", \"item_id\"]]\n",
    "    .groupby(by=\"user_id\")[\"item_id\"]\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"item_id\": \"tfidf_recs\"})\n",
    ")\n",
    "\n",
    "metrics_df_tmp = pd.merge(\n",
    "    metrics_df_tmp,\n",
    "    predictions,\n",
    "    how=\"left\",\n",
    "    on=\"user_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RecommenderMetrics.evaluate_recommender(\n",
    "    metrics_df_tmp,\n",
    "    model_preds_col=\"tfidf_recs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectools LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задаем модель\n",
    "lfm_model = LightFMWrapperModel(\n",
    "    LightFM(\n",
    "        no_components=64,\n",
    "        learning_rate=0.1,\n",
    "        loss=\"warp\",\n",
    "        max_sampled=7,\n",
    "    ),\n",
    "    epochs=20,\n",
    "    num_threads=6,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfm_model.fit(dataset=current_dataset)\n",
    "%clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "with open(models_path + \"lfm_model.dill\", \"wb\") as f:\n",
    "    dill.dump(lfm_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "with open(models_path + \"lfm_model.dill\", \"rb\") as f:\n",
    "    lfm_model = dill.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_lfm = lfm_model.recommend(\n",
    "    b2t_users,\n",
    "    current_dataset,\n",
    "    # выдаем 10 кандидатов\n",
    "    k=10,\n",
    "    # рекомендуем уже просмотренные товары\n",
    "    filter_viewed=False,\n",
    ")\n",
    "candidates_lfm = candidates_lfm.rename(\n",
    "    columns={\n",
    "        \"score\": \"lfm_score\",\n",
    "        \"rank\": \"lfm_rank\",\n",
    "    }\n",
    ")\n",
    "candidates_lfm.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (\n",
    "    candidates_lfm[candidates_lfm[\"lfm_rank\"] <= 10][[\"user_id\", \"item_id\"]]\n",
    "    .groupby(by=\"user_id\")[\"item_id\"]\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"item_id\": \"lfm_recs\"})\n",
    ")\n",
    "\n",
    "metrics_df_tmp = pd.merge(metrics_df_tmp, predictions, how=\"left\", on=\"user_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RecommenderMetrics.evaluate_recommender(\n",
    "    metrics_df_tmp,\n",
    "    model_preds_col=\"lfm_recs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bandit Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mab_model = BanditRecommender(\n",
    "    LearningPolicy.ThompsonSampling(),\n",
    "    top_k=10,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем таблицу данных для моделей первого уровня\n",
    "with open(data_path + \"base_models_data.dill\", \"rb\") as f:\n",
    "    base_models_data = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mab_data = base_models_data[base_models_data[\"user_id\"].isin(b2r_users)]\n",
    "mab_data = mab_data[(mab_data[\"u_total_inter\"] > 20)]\n",
    "mab_data[\"binary_weight\"] = (mab_data[\"ui_inter\"] > 2).astype(int)\n",
    "mab_data = mab_data[\n",
    "    [\n",
    "        \"user_id\",\n",
    "        \"item_id\",\n",
    "        \"dt\",\n",
    "        \"binary_weight\",\n",
    "    ]\n",
    "]\n",
    "mab_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_windows = []\n",
    "min_date = mab_data[\"dt\"].min()\n",
    "max_date = mab_data[\"dt\"].max()\n",
    "\n",
    "\n",
    "cur_min = min_date\n",
    "left, right = cur_min, cur_min + pd.Timedelta(hours=1.5)\n",
    "chunk = mab_data[mab_data[\"dt\"].between(left, right, inclusive=\"left\")]\n",
    "\n",
    "mab_model.fit(\n",
    "    decisions=chunk[\"item_id\"],\n",
    "    rewards=chunk[\"binary_weight\"],\n",
    ")\n",
    "\n",
    "print(f\"Fitted: {left}, {right}\")\n",
    "\n",
    "cur_min = right\n",
    "\n",
    "while right <= max_date:\n",
    "    left, right = cur_min, cur_min + pd.Timedelta(hours=1.5)\n",
    "    chunk = mab_data[mab_data[\"dt\"].between(left, right, inclusive=\"left\")]\n",
    "    mab_model.partial_fit(\n",
    "        decisions=chunk[\"item_id\"],\n",
    "        rewards=chunk[\"binary_weight\"],\n",
    "    )\n",
    "    cur_min = right\n",
    "    print(f\"Fitted: {left}, {right}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "with open(models_path + \"mab_model.dill\", \"wb\") as f:\n",
    "    dill.dump(mab_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "with open(models_path + \"mab_model.dill\", \"rb\") as f:\n",
    "    mab_model: BanditRecommender = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNBanditRecommender:\n",
    "    \"\"\"\n",
    "    Class for recommending items with Multi-Armed Bandit\n",
    "    and knn model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: RTDataset,\n",
    "        path_bandit_model: str,\n",
    "        path_knn_model: str,\n",
    "        path_popular_model: str,\n",
    "    ):\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "        with open(path_bandit_model, \"rb\") as f:\n",
    "            self.mab_model: BanditRecommender = dill.load(f)\n",
    "\n",
    "        with open(path_knn_model, \"rb\") as f:\n",
    "            self.knn_model: implicit_knn = dill.load(f)\n",
    "\n",
    "        with open(path_popular_model, \"rb\") as f:\n",
    "            self.popular_model: implicit_knn = dill.load(f)\n",
    "\n",
    "    def __get_arms_for_users(self, user_ids):\n",
    "        candidates_knn = self.knn_model.recommend(\n",
    "            user_ids,\n",
    "            self.dataset,\n",
    "            # выдаем 25 кандидатов\n",
    "            # из которых будет выбирать бандит\n",
    "            k=25,\n",
    "            # рекомендуем уже просмотренные товары\n",
    "            filter_viewed=False,\n",
    "        )\n",
    "\n",
    "        return candidates_knn[[\"user_id\", \"item_id\"]]\n",
    "\n",
    "    def predict(self, user_ids: npt.ArrayLike):\n",
    "\n",
    "        recs = pd.DataFrame()\n",
    "        cur_recs = pd.DataFrame()\n",
    "\n",
    "        candidates_knn = self.__get_arms_for_users(user_ids)\n",
    "\n",
    "        candidates_pop = self.popular_model.recommend(\n",
    "            [user_ids[0]],\n",
    "            self.dataset,\n",
    "            # выдаем 50 кандидатов\n",
    "            k=50,\n",
    "            # рекомендуем уже просмотренные товары\n",
    "            filter_viewed=False,\n",
    "        )[\"item_id\"].values\n",
    "\n",
    "        print(\"KNN predicted\")\n",
    "\n",
    "        for user_id in tqdm(user_ids):\n",
    "            try:\n",
    "                filtered_arms = candidates_knn[candidates_knn[\"user_id\"] == user_id][\n",
    "                    \"item_id\"\n",
    "                ].values\n",
    "                if len(filtered_arms) < self.mab_model.top_k:\n",
    "                    filtered_arms = np.concatenate(\n",
    "                        [\n",
    "                            filtered_arms,\n",
    "                            np.random.choice(\n",
    "                                candidates_pop,\n",
    "                                size=25,\n",
    "                                replace=False,\n",
    "                            ),\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "                self.mab_model.set_arms(filtered_arms)\n",
    "                mab_recs = self.mab_model.recommend(return_scores=True)\n",
    "                cur_recs[\"user_id\"] = [user_id] * self.mab_model.top_k\n",
    "                cur_recs[\"item_id\"] = mab_recs[0]\n",
    "                cur_recs[\"mab_score\"] = mab_recs[1]\n",
    "                cur_recs[\"mab_rank\"] = [i for i in range(1, 11)]\n",
    "\n",
    "                recs = pd.concat([recs, cur_recs])\n",
    "            except Exception as e:\n",
    "                print(filtered_arms)\n",
    "                print(user_id)\n",
    "                print(mab_recs)\n",
    "                raise e\n",
    "\n",
    "        return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_bandit_model = KNNBanditRecommender(\n",
    "    dataset=current_dataset,\n",
    "    path_bandit_model=models_path + \"mab_model.dill\",\n",
    "    path_knn_model=models_path + \"knn_impl_bm25_k50.dill\",\n",
    "    path_popular_model=models_path + \"popular_model.dill\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = knn_bandit_model.predict(b2t_users)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_mab = predictions\n",
    "candidates_mab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (\n",
    "    candidates_mab[candidates_mab[\"mab_rank\"] <= 10][[\"user_id\", \"item_id\"]]\n",
    "    .groupby(by=\"user_id\")[\"item_id\"]\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"item_id\": \"mab_recs\"})\n",
    ")\n",
    "\n",
    "metrics_df_tmp = pd.merge(metrics_df_tmp, predictions, how=\"left\", on=\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RecommenderMetrics.evaluate_recommender(\n",
    "    metrics_df_tmp,\n",
    "    model_preds_col=\"mab_recs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Наблюдения по моделям первого уровня"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|       **Модель**       | **Время обучения** | **Время иференса (100 тыс. пользователей)** | **ndcg** | **recall** |\n",
    "|:----------------------:|:------------------:|:-------------------------------------------:|----------|------------|\n",
    "|      **PopularK**      |        6.6 с       |                    0.8 с                    |  0.0126  |   0.0021   |\n",
    "|  **knn_impl_cos_k50**  |       14.25 с      |                    12.5 с                   |  0.2566  |   0.0465   |\n",
    "|  **knn_impl_bm25_k50** |       13.25 с      |                    11.5 с                   |  0.2517  |   0.0459   |\n",
    "| **knn_impl_tdidf_k50** |       13.25 с      |                    11.5 с                   |  0.256   |   0.0461   |\n",
    "| **LightFM (epoch=20)** |       7 м 15 с     |                     1 м                     |   0.165  |   0.0279   |\n",
    "| **Bandit Recommender** |        14 м        |                     10 м                    |   0.147  |   0.0257   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из таблицы видно, что knn based алгоритмы хорошо и быстро обучаются на полном датасете, когда lightFM модель требует большего времени как для обучения, так и для инференса.\n",
    "\n",
    "Bandit Recommender в коопе с KNN в принципе перспективная связка, но времени для обучения и инференса требует много, скорее всего откажусь от танного типа модели. \n",
    "\n",
    "DL модели в качестве моделей первого уровня решил не запускать: \n",
    "- мало временных зависимостей: все таки если логически размышлять, то блуждание пользователя по товарам в ленте тем более за 2 дня --- это слишком волатильный процесс\n",
    "- так же для DL модели лучше использовать фитчи предметов или пользователей, но их было решено оставить под обучение ранкера\n",
    "\n",
    "Для автоэнкодеров --- слишком большой датасет, было бы нормально, если пользователей можно было бы разнести по кластерам, но для этого у нас нет данных о самих пользователях.\n",
    "\n",
    "\n",
    "### ИТОГО\n",
    "\n",
    "Оставляем PopularModel, kNN-based методы и LightFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получим рекомендации для обучения ранкера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_data_path = models_path + \"candidates_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PopularModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PopularModel\n",
    "with open(models_path + \"popular_model.dill\", \"rb\") as f:\n",
    "    popular_model: PopularModel = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_pop = popular_model.recommend(\n",
    "    b2r_users,\n",
    "    current_dataset,\n",
    "    # выдаем 20 кандидатов\n",
    "    k=20,\n",
    "    # рекомендуем уже просмотренные товары\n",
    "    filter_viewed=False,\n",
    ")\n",
    "\n",
    "candidates_pop = candidates_pop.rename(\n",
    "    columns={\n",
    "        \"score\": \"pop_score\",\n",
    "        \"rank\": \"pop_rank\",\n",
    "    }\n",
    ")\n",
    "\n",
    "candidates_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PopularModel candidates\n",
    "with open(candidates_data_path + \"candidates_pop.dill\", \"wb\") as f:\n",
    "    dill.dump(candidates_pop, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Recommender\n",
    "with open(models_path + \"knn_impl_cos_k50.dill\", \"rb\") as f:\n",
    "    knn_impl_cos_k50 = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_cos = knn_impl_cos_k50.recommend(\n",
    "    b2r_users,\n",
    "    current_dataset,\n",
    "    # выдаем 20 кандидатов\n",
    "    k=20,\n",
    "    # рекомендуем уже просмотренные товары\n",
    "    filter_viewed=False,\n",
    ")\n",
    "candidates_cos = candidates_cos.rename(\n",
    "    columns={\n",
    "        \"score\": \"cos_score\",\n",
    "        \"rank\": \"cos_rank\",\n",
    "    }\n",
    ")\n",
    "\n",
    "candidates_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Cosine Model candidates\n",
    "with open(candidates_data_path + \"candidates_cos.dill\", \"wb\") as f:\n",
    "    dill.dump(candidates_cos, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25 Recommender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 Recommender\n",
    "with open(models_path + \"knn_impl_bm25_k50.dill\", \"rb\") as f:\n",
    "    knn_impl_bm25_k50 = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_bm25 = knn_impl_bm25_k50.recommend(\n",
    "    b2r_users,\n",
    "    current_dataset,\n",
    "    # выдаем 20 кандидатов\n",
    "    k=20,\n",
    "    # рекомендуем уже просмотренные товары\n",
    "    filter_viewed=False,\n",
    ")\n",
    "candidates_bm25 = candidates_bm25.rename(\n",
    "    columns={\n",
    "        \"score\": \"bm25_score\",\n",
    "        \"rank\": \"bm25_rank\",\n",
    "    }\n",
    ")\n",
    "candidates_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save BM25 Model candidates\n",
    "with open(candidates_data_path + \"candidates_bm25.dill\", \"wb\") as f:\n",
    "    dill.dump(candidates_bm25, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Recommender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF Recommender\n",
    "with open(models_path + \"knn_impl_tfidf_k50.dill\", \"rb\") as f:\n",
    "    knn_impl_tfidf_k50 = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_tfidf = knn_impl_tfidf_k50.recommend(\n",
    "    b2r_users,\n",
    "    current_dataset,\n",
    "    # выдаем 20 кандидатов\n",
    "    k=20,\n",
    "    # рекомендуем уже просмотренные товары\n",
    "    filter_viewed=False,\n",
    ")\n",
    "candidates_tfidf = candidates_tfidf.rename(\n",
    "    columns={\n",
    "        \"score\": \"tfidf_score\",\n",
    "        \"rank\": \"tfidf_rank\",\n",
    "    }\n",
    ")\n",
    "candidates_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TFIDF Model candidates\n",
    "with open(candidates_data_path + \"candidates_tfidf.dill\", \"wb\") as f:\n",
    "    dill.dump(candidates_tfidf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightFM Recommender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightFM Recommender\n",
    "with open(models_path + \"lfm_model.dill\", \"rb\") as f:\n",
    "    lfm_model = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_lfm = lfm_model.recommend(\n",
    "    b2r_users,\n",
    "    current_dataset,\n",
    "    # выдаем 20 кандидатов\n",
    "    k=20,\n",
    "    # рекомендуем уже просмотренные товары\n",
    "    filter_viewed=False,\n",
    ")\n",
    "candidates_lfm = candidates_lfm.rename(\n",
    "    columns={\n",
    "        \"score\": \"lfm_score\",\n",
    "        \"rank\": \"lfm_rank\",\n",
    "    }\n",
    ")\n",
    "candidates_lfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LightFM Model candidates\n",
    "with open(candidates_data_path + \"candidates_lfm.dill\", \"wb\") as f:\n",
    "    dill.dump(candidates_lfm, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сливаем всех кандидатов в одну таблицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = \"../models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_data_path = models_path + \"candidates_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как LightFM умеет работать с warm и cold пользователями (PopularModel была взята для тех же целей), а PopularModel имеет **плохой score** относительно остальных моделей и **сильно увеличивает размерность** получаемого датасета с кандидатами, то от кандидатов PopularModel решено отказаться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(candidates_data_path + \"candidates_cos.dill\", \"rb\") as f:\n",
    "    candidates_cos = dill.load(f)\n",
    "    candidates_cos = candidates_cos[candidates_cos[\"cos_rank\"] < 15]\n",
    "\n",
    "with open(candidates_data_path + \"candidates_bm25.dill\", \"rb\") as f:\n",
    "    candidates_bm25 = dill.load(f)\n",
    "    candidates_bm25 = candidates_bm25[candidates_bm25[\"bm25_rank\"] < 15]\n",
    "\n",
    "with open(candidates_data_path + \"candidates_tfidf.dill\", \"rb\") as f:\n",
    "    candidates_tfidf = dill.load(f)\n",
    "    candidates_tfidf = candidates_tfidf[candidates_tfidf[\"tfidf_rank\"] < 15]\n",
    "\n",
    "with open(candidates_data_path + \"candidates_lfm.dill\", \"rb\") as f:\n",
    "    candidates_lfm = dill.load(f)\n",
    "    candidates_lfm = candidates_lfm[candidates_lfm[\"lfm_rank\"] < 15]\n",
    "\n",
    "# with open(candidates_data_path + \"candidates_pop.dill\", \"rb\") as f:\n",
    "#     candidates_pop = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_list = [\n",
    "    candidates_cos,\n",
    "    candidates_bm25,\n",
    "    candidates_tfidf,\n",
    "    candidates_lfm,\n",
    "    # candidates_pop,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in candidates_list:\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = candidates_list[0].copy()\n",
    "\n",
    "for df in candidates_list[1:]:\n",
    "    candidates = pd.concat(\n",
    "        [\n",
    "            candidates.set_index([\"user_id\", \"item_id\"]),\n",
    "            df.set_index([\"user_id\", \"item_id\"]),\n",
    "        ],\n",
    "        join=\"outer\",\n",
    "        axis=1,\n",
    "    ).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check shape\n",
    "candidates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_values_merged = {\n",
    "    \"cos_score\": candidates[\"cos_score\"].min() - 0.01,\n",
    "    \"bm25_score\": candidates[\"bm25_score\"].min() - 0.01,\n",
    "    \"tfidf_score\": candidates[\"tfidf_score\"].min() - 0.01,\n",
    "    \"lfm_score\": candidates[\"lfm_score\"].min() - 0.01,\n",
    "    \"cos_rank\": candidates[\"cos_rank\"].max() + 1,\n",
    "    \"bm25_rank\": candidates[\"bm25_rank\"].max() + 1,\n",
    "    \"tfidf_rank\": candidates[\"tfidf_rank\"].max() + 1,\n",
    "    \"lfm_rank\": candidates[\"lfm_rank\"].max() + 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates.fillna(default_values_merged, inplace=True)\n",
    "candidates.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "with open(candidates_data_path + \"candidates_full.dill\", \"wb\") as f:\n",
    "    dill.dump(candidates, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
