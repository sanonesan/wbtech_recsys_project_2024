{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WB RecSys Project\n",
    "\n",
    "# Общее описание проекта\n",
    "\n",
    "Необходимо на основании взаимодействий пользователей с товарами предсказать следующие взаимодействия пользователей с товарами.\n",
    "\n",
    "# Stage 3\n",
    "\n",
    "- Проработать проблематику (в чем смысл для бизнеса)\n",
    "- Грамотно формализовать задачу\n",
    "- Проанализировать имеющиеся данные и оценить их пригодность для решения поставленной задачи\n",
    "- Провести первичный разведочный анализ данных (EDA)\n",
    "\n",
    "# Preprocessing images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорт библиотек"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import dill\n",
    "import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Путь до данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../data_closed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтение данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим для каких айтемов отсутствуют картинки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + \"df_items.dill\", \"rb\") as f:\n",
    "    df_items = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorded list with existing items\n",
    "item_id_list = sorted(df_items[\"item_id\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_without_img = []\n",
    "\n",
    "for i in item_id_list:\n",
    "    if not (os.path.exists(data_path + \"images/\" + f\"{i}.jpg\")):\n",
    "        items_without_img.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(items_without_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всего таких айтемов 269"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP: Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получим отсортированный список лейблов изображений,\n",
    "# для которых присутствуют картиики\n",
    "labels = sorted(list(set(item_id_list) - set(items_without_img)))\n",
    "\n",
    "images_paths = [\n",
    "    data_path + \"images/\" + f\"{idx}.jpg\" for idx in labels\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Размер изображения, к которому будем приводить\n",
    "IMG_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зададим свой датасет\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_paths,\n",
    "        labels,\n",
    "        transform=None,\n",
    "        image_size: int= 128,\n",
    "    ):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.file_paths[index]).resize(\n",
    "            size=(self.image_size, self.image_size),\n",
    "        )\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем датасет\n",
    "imgs_dataset = CustomImageDataset(\n",
    "    images_paths[:128  * 20],\n",
    "    labels=labels[:128  * 20],\n",
    "    image_size=IMG_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image embeddings\n",
    "\n",
    "Extract image embeddings on both subsets using pretrained CLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model\n",
    "model = CLIPModel.from_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\",\n",
    ").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\",\n",
    ")\n",
    "\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgs_collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    images = [\n",
    "        processor(\n",
    "            images=image,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"pixel_values\"]\n",
    "        for image in images\n",
    "    ]\n",
    "    images = torch.cat(images, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "# Get dataloader\n",
    "imgs_loader = torch.utils.data.DataLoader(\n",
    "    imgs_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=imgs_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test\n",
    "batch = next(iter(imgs_loader))\n",
    "with torch.no_grad():\n",
    "    out = model.get_image_features(batch[0].to(device))\n",
    "    lab = batch[1]\n",
    "\n",
    "display(out.shape)\n",
    "display(lab.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(\n",
    "    model, loader, device=device\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Отключаем градиенты для ускорения\n",
    "        for images, labels in tqdm.tqdm(iter(loader)):\n",
    "            # Перемещаем данные на указанное устройство\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Извлекаем эмбеддинги из модели\n",
    "            embeddings = model.get_image_features(images)\n",
    "\n",
    "            # Сохраняем результаты\n",
    "            all_embeddings.append(embeddings)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    # Объединяем все батчи в один тензор\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0).to(\"cpu\")\n",
    "    all_labels = torch.cat(all_labels, dim=0).to(\"cpu\")\n",
    "\n",
    "    return all_embeddings, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embeds, img_labels = extract_embeddings(model, imgs_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_labels, img_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embeds, img_embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Снизим размерность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_to_keep = 25\n",
    "\n",
    "pca_lowrank = PCA(n_components=components_to_keep)\n",
    "all_embeddings = pca_lowrank.fit_transform(img_embeds)\n",
    "all_embeddings.shape\n",
    "img_embeds_df = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(\n",
    "            img_labels,\n",
    "            columns=[\"item_id\"],\n",
    "        ).reset_index(drop=True),\n",
    "        pd.DataFrame(\n",
    "            all_embeddings,\n",
    "            columns=[f\"pca_img_{i}\" for i in range(components_to_keep)],\n",
    "        ).reset_index(drop=True),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "img_embeds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
